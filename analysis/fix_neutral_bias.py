#!/usr/bin/env python3
"""
NEUTRAL í¸í–¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ëª¨ë¸ ì¬í›ˆë ¨
- íƒ€ì„í”„ë ˆì„ë³„ ì ì ˆí•œ ì„ê³„ê°’ ì‚¬ìš©
- í´ë˜ìŠ¤ ê· í˜• ê°œì„ 
- ì‹¤ì œ ê±°ë˜ ê°€ëŠ¥í•œ ì‹ í˜¸ ìƒì„±
"""

import ccxt
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import accuracy_score, classification_report
from sklearn.utils.class_weight import compute_class_weight
import joblib
from datetime import datetime
from loguru import logger
import warnings
warnings.filterwarnings('ignore')

class ImprovedTradingModels:
    def __init__(self):
        self.exchange = ccxt.binance()

        # íƒ€ì„í”„ë ˆì„ë³„ ì ì ˆí•œ ì„ê³„ê°’
        self.thresholds = {
            '15m': 0.001,  # 0.1% (ê¸°ì¡´ 0.2% â†’ 0.1%)
            '30m': 0.0015,  # 0.15%
            '4h': 0.003,   # 0.3%
            '1d': 0.005    # 0.5%
        }

        self.data_limits = {
            '15m': 3000,
            '30m': 2000,
            '4h': 1000,
            '1d': 500
        }

    def create_balanced_labels(self, df, timeframe):
        """ê· í˜•ì¡íŒ ë¼ë²¨ ìƒì„±"""
        threshold = self.thresholds[timeframe]

        # ë¯¸ë˜ ìˆ˜ìµë¥  ê³„ì‚°
        future_return = df['close'].shift(-1) / df['close'] - 1

        # ë¼ë²¨ ìƒì„±
        labels = pd.Series(1, index=df.index)  # ê¸°ë³¸ê°’ NEUTRAL
        labels[future_return > threshold] = 2   # LONG
        labels[future_return < -threshold] = 0  # SHORT

        # ë¼ë²¨ ë¶„í¬ ì¶œë ¥
        label_counts = labels.value_counts().sort_index()
        total = len(labels.dropna())

        logger.info(f"  ë¼ë²¨ ë¶„í¬ ({timeframe}, ì„ê³„ê°’ {threshold*100:.2f}%):")
        for label, count in label_counts.items():
            label_name = {0: 'SHORT', 1: 'NEUTRAL', 2: 'LONG'}[label]
            pct = (count / total) * 100
            logger.info(f"    {label_name}: {count:4d}ê°œ ({pct:5.1f}%)")

        return labels

    def get_data(self, timeframe):
        """ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        limit = self.data_limits[timeframe]

        logger.info(f"  ë°ì´í„° ìˆ˜ì§‘ ì¤‘... ({timeframe}, {limit}ê°œ ìº”ë“¤)")

        ohlcv = self.exchange.fetch_ohlcv('BTC/USDT', timeframe=timeframe, limit=limit)
        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)

        return df

    def create_features(self, df):
        """í–¥ìƒëœ íŠ¹ì§• ìƒì„±"""
        features = pd.DataFrame(index=df.index)

        # 1. ê°€ê²© ë³€í™”ìœ¨ (ë‹¤ì–‘í•œ ê¸°ê°„)
        for period in [1, 2, 3, 5, 7, 10, 15, 20]:
            features[f'return_{period}'] = df['close'].pct_change(period) * 100

        # 2. RSI (ë‹¤ì–‘í•œ ê¸°ê°„)
        for period in [7, 14, 21, 28]:
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss.replace(0, 1)
            features[f'rsi_{period}'] = 100 - (100 / (1 + rs))

        # 3. MACD
        exp1 = df['close'].ewm(span=12, adjust=False).mean()
        exp2 = df['close'].ewm(span=26, adjust=False).mean()
        features['macd'] = exp1 - exp2
        features['macd_signal'] = features['macd'].ewm(span=9, adjust=False).mean()
        features['macd_histogram'] = features['macd'] - features['macd_signal']

        # 4. ë³¼ë¦°ì € ë°´ë“œ
        for period in [10, 20, 30]:
            sma = df['close'].rolling(window=period).mean()
            std = df['close'].rolling(window=period).std()
            features[f'bb_width_{period}'] = (std * 2) / sma * 100
            features[f'bb_position_{period}'] = (df['close'] - sma) / (std * 2)

        # 5. ë³¼ë¥¨ ì§€í‘œ
        features['volume_ratio'] = df['volume'] / df['volume'].rolling(window=20).mean()
        features['volume_change'] = df['volume'].pct_change() * 100

        # 6. ë³€ë™ì„±
        features['volatility'] = df['close'].pct_change().rolling(window=20).std() * 100

        # 7. ê³ ì € ë¹„ìœ¨
        features['high_low_ratio'] = (df['high'] - df['low']) / df['close'] * 100

        # 8. ì´ë™í‰ê· 
        for period in [5, 10, 20, 50]:
            ma = df['close'].rolling(window=period).mean()
            features[f'ma_{period}_ratio'] = df['close'] / ma

        return features.fillna(0)

    def train_model(self, timeframe):
        """ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨"""
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸš€ {timeframe} ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        logger.info(f"{'='*60}")

        # ë°ì´í„° ìˆ˜ì§‘
        df = self.get_data(timeframe)

        # íŠ¹ì§• ë° ë¼ë²¨ ìƒì„±
        features = self.create_features(df)
        labels = self.create_balanced_labels(df, timeframe)

        # ì •ë ¬ ë° ì •ë¦¬
        X = features.dropna()
        y = labels[X.index][:-1]  # ë§ˆì§€ë§‰ ë¼ë²¨ ì œì™¸ (ë¯¸ë˜ ë°ì´í„° ì—†ìŒ)
        X = X[:-1]

        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í•  (ì‹œê³„ì—´ ìœ ì§€)
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]

        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        classes = np.unique(y_train)
        class_weights = compute_class_weight('balanced', classes=classes, y=y_train)
        class_weight_dict = dict(zip(classes, class_weights))

        logger.info(f"  í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weight_dict}")

        # ìŠ¤ì¼€ì¼ë§
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # ì•™ìƒë¸” ëª¨ë¸ ìƒì„±
        rf = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=10,
            min_samples_leaf=5,
            class_weight=class_weight_dict,
            random_state=42,
            n_jobs=-1
        )

        gb = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=7,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=42
        )

        # ì•™ìƒë¸”
        model = VotingClassifier(
            estimators=[('rf', rf), ('gb', gb)],
            voting='soft',
            weights=[0.6, 0.4]
        )

        # í›ˆë ¨
        logger.info("  ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        model.fit(X_train_scaled, y_train)

        # í‰ê°€
        y_pred = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)

        logger.info(f"\n  ì „ì²´ ì •í™•ë„: {accuracy*100:.2f}%")

        # ì˜ˆì¸¡ ë¶„í¬ í™•ì¸
        pred_counts = pd.Series(y_pred).value_counts().sort_index()
        total_pred = len(y_pred)

        logger.info(f"\n  ì˜ˆì¸¡ ë¶„í¬:")
        for label, count in pred_counts.items():
            label_name = {0: 'SHORT', 1: 'NEUTRAL', 2: 'LONG'}[label]
            pct = (count / total_pred) * 100
            logger.info(f"    {label_name}: {count:4d}ê°œ ({pct:5.1f}%)")

        # ë†’ì€ ì‹ ë¢°ë„ ì˜ˆì¸¡ ë¶„ì„
        y_pred_proba = model.predict_proba(X_test_scaled)
        high_conf_mask = np.max(y_pred_proba, axis=1) >= 0.7

        if high_conf_mask.any():
            high_conf_accuracy = accuracy_score(y_test[high_conf_mask], y_pred[high_conf_mask])
            high_conf_count = high_conf_mask.sum()
            high_conf_pct = (high_conf_count / len(y_pred)) * 100

            logger.info(f"\n  ê³ ì‹ ë¢°ë„ (â‰¥70%) ì˜ˆì¸¡:")
            logger.info(f"    ê°œìˆ˜: {high_conf_count}ê°œ ({high_conf_pct:.1f}%)")
            logger.info(f"    ì •í™•ë„: {high_conf_accuracy*100:.2f}%")

            # ê³ ì‹ ë¢°ë„ ì˜ˆì¸¡ ë¶„í¬
            high_conf_pred = y_pred[high_conf_mask]
            high_conf_counts = pd.Series(high_conf_pred).value_counts().sort_index()

            logger.info(f"    ë¶„í¬:")
            for label, count in high_conf_counts.items():
                label_name = {0: 'SHORT', 1: 'NEUTRAL', 2: 'LONG'}[label]
                pct = (count / len(high_conf_pred)) * 100
                logger.info(f"      {label_name}: {count:3d}ê°œ ({pct:5.1f}%)")

        # ë¶„ë¥˜ ë³´ê³ ì„œ
        logger.info(f"\n  ë¶„ë¥˜ ë³´ê³ ì„œ:")
        report = classification_report(y_test, y_pred,
                                      target_names=['SHORT', 'NEUTRAL', 'LONG'])
        for line in report.split('\n'):
            if line:
                logger.info(f"    {line}")

        # ëª¨ë¸ ì €ì¥
        model_file = f'models/fixed_{timeframe.replace("m", "min").replace("h", "hour").replace("d", "day")}_model.pkl'
        scaler_file = f'models/fixed_{timeframe.replace("m", "min").replace("h", "hour").replace("d", "day")}_scaler.pkl'

        joblib.dump(model, model_file)
        joblib.dump(scaler, scaler_file)

        logger.success(f"  âœ… ëª¨ë¸ ì €ì¥: {model_file}")

        return accuracy, model, scaler

    def test_realtime_predictions(self, model, scaler, timeframe):
        """ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
        logger.info(f"\n  ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸:")

        # ìµœì‹  ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
        df = self.get_data(timeframe)
        features = self.create_features(df)

        # ìµœê·¼ 10ê°œ ì˜ˆì¸¡
        recent_features = features.iloc[-10:]
        recent_scaled = scaler.transform(recent_features)

        predictions = model.predict(recent_scaled)
        probabilities = model.predict_proba(recent_scaled)

        for i in range(len(predictions)):
            pred = predictions[i]
            prob = probabilities[i]
            max_prob = max(prob) * 100

            label_name = {0: 'SHORT', 1: 'NEUTRAL', 2: 'LONG'}[pred]
            timestamp = recent_features.index[i].strftime('%m-%d %H:%M')

            logger.info(f"    {timestamp}: {label_name:8s} (ì‹ ë¢°ë„: {max_prob:.1f}%)")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    trainer = ImprovedTradingModels()

    logger.info("="*70)
    logger.info("ğŸ”§ NEUTRAL í¸í–¥ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ëª¨ë¸ ì¬í›ˆë ¨")
    logger.info("="*70)
    logger.info("")
    logger.info("ë³€ê²½ ì‚¬í•­:")
    logger.info("- 15ë¶„: 0.2% â†’ 0.1% ì„ê³„ê°’")
    logger.info("- 30ë¶„: 0.2% â†’ 0.15% ì„ê³„ê°’")
    logger.info("- 4ì‹œê°„: 0.2% â†’ 0.3% ì„ê³„ê°’")
    logger.info("- 1ì¼: 0.2% â†’ 0.5% ì„ê³„ê°’")
    logger.info("- í´ë˜ìŠ¤ ê· í˜• ê°€ì¤‘ì¹˜ ì ìš©")
    logger.info("")

    results = {}

    # ê° íƒ€ì„í”„ë ˆì„ ëª¨ë¸ í›ˆë ¨
    for timeframe in ['15m', '30m', '4h', '1d']:
        accuracy, model, scaler = trainer.train_model(timeframe)
        trainer.test_realtime_predictions(model, scaler, timeframe)
        results[timeframe] = accuracy

    # ìµœì¢… ê²°ê³¼
    logger.info("\n" + "="*70)
    logger.info("ğŸ“Š ìµœì¢… ê²°ê³¼")
    logger.info("="*70)

    for timeframe, accuracy in results.items():
        logger.info(f"  {timeframe:4s}: {accuracy*100:.2f}% ì •í™•ë„")

    logger.success("\nâœ… ëª¨ë“  ëª¨ë¸ ì¬í›ˆë ¨ ì™„ë£Œ!")
    logger.info("ìƒˆë¡œìš´ ëª¨ë¸ë“¤ì´ models/fixed_*.pklë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    logger.info("ì´ì œ ì‹¤ì œ ê±°ë˜ ì‹ í˜¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()