#!/usr/bin/env python3
"""
ëª¨ë¸ì˜ ì‹¤ì œ ì˜ˆì¸¡ ë¶„í¬ í™•ì¸ - ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import pandas as pd
import numpy as np
import joblib
from datetime import datetime, timedelta
import ccxt
from loguru import logger

# ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
def load_models():
    models = {}
    scalers = {}

    model_files = {
        '15m': ('models/main_15m_model.pkl', 'models/main_15m_scaler.pkl'),
        '30m': ('models/main_30m_model.pkl', 'models/main_30m_scaler.pkl'),
        '4h': ('models/trend_4h_model.pkl', 'models/trend_4h_scaler.pkl'),
        '1d': ('models/trend_1d_model.pkl', 'models/trend_1d_scaler.pkl')
    }

    for timeframe, (model_file, scaler_file) in model_files.items():
        if os.path.exists(model_file) and os.path.exists(scaler_file):
            models[timeframe] = joblib.load(model_file)
            scalers[timeframe] = joblib.load(scaler_file)
            print(f"âœ… {timeframe} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        else:
            print(f"âŒ {timeframe} ëª¨ë¸ íŒŒì¼ ì—†ìŒ")

    return models, scalers

# ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
def get_historical_data(timeframe='15m', limit=1000):
    exchange = ccxt.binance()

    # íƒ€ìž„í”„ë ˆìž„ ë§¤í•‘
    tf_map = {
        '15m': '15m',
        '30m': '30m',
        '4h': '4h',
        '1d': '1d'
    }

    try:
        ohlcv = exchange.fetch_ohlcv(
            'BTC/USDT',
            timeframe=tf_map[timeframe],
            limit=limit
        )

        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)

        return df
    except Exception as e:
        print(f"ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return None

# íŠ¹ì§• ìƒì„± (15ë¶„ ëª¨ë¸ìš©)
def prepare_basic_features(df):
    features = pd.DataFrame(index=df.index)

    # ê°€ê²© ë³€í™”ìœ¨
    for period in [1, 3, 5, 10]:
        features[f'return_{period}'] = df['close'].pct_change(period) * 100

    # RSI
    for period in [7, 14, 21]:
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss.replace(0, 1)
        features[f'rsi_{period}'] = 100 - (100 / (1 + rs))

    # MACD
    exp1 = df['close'].ewm(span=12, adjust=False).mean()
    exp2 = df['close'].ewm(span=26, adjust=False).mean()
    features['macd'] = exp1 - exp2
    features['macd_signal'] = features['macd'].ewm(span=9, adjust=False).mean()

    # ë³¼ë¦°ì € ë°´ë“œ
    for period in [10, 20]:
        sma = df['close'].rolling(window=period).mean()
        std = df['close'].rolling(window=period).std()
        features[f'bb_width_{period}'] = (std * 2) / sma * 100
        features[f'bb_position_{period}'] = (df['close'] - sma) / (std * 2)

    # ë³¼ë¥¨
    features['volume_ratio'] = df['volume'] / df['volume'].rolling(window=20).mean()
    features['volume_change'] = df['volume'].pct_change() * 100

    # High-Low ë¹„ìœ¨
    features['high_low_ratio'] = (df['high'] - df['low']) / df['close'] * 100

    return features.fillna(0)

# ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
def test_predictions(models, scalers):
    results = {}

    for timeframe in ['15m', '30m', '4h', '1d']:
        if timeframe not in models:
            continue

        print(f"\n{'='*60}")
        print(f"ðŸ“Š {timeframe} ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        print(f"{'='*60}")

        # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        df = get_historical_data(timeframe, limit=500)
        if df is None:
            continue

        # íŠ¹ì§• ìƒì„± (ê°„ë‹¨í™”)
        features = prepare_basic_features(df)

        # ìµœê·¼ 100ê°œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
        test_features = features.iloc[-100:]

        # ì˜ˆì¸¡ ë¶„í¬ í™•ì¸
        predictions = []
        probabilities = []

        for i in range(len(test_features)):
            try:
                # ìŠ¤ì¼€ì¼ë§
                X = test_features.iloc[i:i+1]

                # 15m ëª¨ë¸ì€ 16ê°œ íŠ¹ì§• ì„ íƒ
                if timeframe == '15m':
                    feature_cols = ['return_1', 'return_3', 'return_5', 'return_10',
                                  'rsi_7', 'rsi_14', 'rsi_21', 'macd', 'macd_signal',
                                  'bb_width_10', 'bb_width_20', 'bb_position_10',
                                  'bb_position_20', 'volume_ratio', 'volume_change',
                                  'high_low_ratio']
                    X = X[feature_cols]

                # 30m ëª¨ë¸ì€ 30ê°œ íŠ¹ì§• í•„ìš” - ì‹¤ì œ ëª¨ë¸ê³¼ ë§žì¶°ì•¼ í•¨
                elif timeframe == '30m':
                    # 30ë¶„ ëª¨ë¸ì€ íŠ¹ë³„ ì²˜ë¦¬ í•„ìš”
                    continue

                # ìŠ¤ì¼€ì¼ë§
                X_scaled = scalers[timeframe].transform(X)

                # ì˜ˆì¸¡
                pred = models[timeframe].predict(X_scaled)[0]
                pred_proba = models[timeframe].predict_proba(X_scaled)[0]

                predictions.append(pred)
                probabilities.append(max(pred_proba))

            except Exception as e:
                continue

        if predictions:
            # ê²°ê³¼ ë¶„ì„
            unique, counts = np.unique(predictions, return_counts=True)
            total = len(predictions)

            print(f"ì´ {total}ê°œ ì˜ˆì¸¡:")

            # ì‹ í˜¸ ë§¤í•‘
            signal_map = {0: 'SHORT', 1: 'NEUTRAL', 2: 'LONG'}

            for val, count in zip(unique, counts):
                signal = signal_map.get(val, f'Unknown({val})')
                percentage = (count / total) * 100
                print(f"  {signal:8s}: {count:3d}íšŒ ({percentage:5.1f}%)")

            # í‰ê·  ì‹ ë¢°ë„
            if probabilities:
                avg_prob = np.mean(probabilities) * 100
                print(f"\ní‰ê·  ì‹ ë¢°ë„: {avg_prob:.1f}%")

            # ìµœê·¼ 10ê°œ ì˜ˆì¸¡
            print(f"\nìµœê·¼ 10ê°œ ì˜ˆì¸¡:")
            for i, (pred, prob) in enumerate(list(zip(predictions[-10:], probabilities[-10:])), 1):
                signal = signal_map.get(pred, f'Unknown({pred})')
                print(f"  {i:2d}. {signal:8s} (ì‹ ë¢°ë„: {prob*100:.1f}%)")

            results[timeframe] = {
                'predictions': predictions,
                'distribution': dict(zip(unique, counts))
            }

    return results

# ë¬¸ì œ ì§„ë‹¨
def diagnose_neutral_bias():
    print("\n" + "="*60)
    print("ðŸ” NEUTRAL íŽ¸í–¥ ë¬¸ì œ ì§„ë‹¨")
    print("="*60)

    # 15ë¶„ ëª¨ë¸ ìƒì„¸ ë¶„ì„
    model_15m = joblib.load('models/main_15m_model.pkl')
    scaler_15m = joblib.load('models/main_15m_scaler.pkl')

    # ìž„ê³„ê°’ í™•ì¸
    print("\n1. ëª¨ë¸ í´ëž˜ìŠ¤ ë¶„í¬:")
    if hasattr(model_15m, 'classes_'):
        print(f"   í´ëž˜ìŠ¤: {model_15m.classes_}")

    if hasattr(model_15m, 'class_weight_'):
        print(f"   í´ëž˜ìŠ¤ ê°€ì¤‘ì¹˜: {model_15m.class_weight_}")

    # íŠ¹ì§• ì¤‘ìš”ë„
    if hasattr(model_15m, 'feature_importances_'):
        print(f"\n2. íŠ¹ì§• ì¤‘ìš”ë„ ìƒìœ„ 5ê°œ:")
        feature_names = ['return_1', 'return_3', 'return_5', 'return_10',
                        'rsi_7', 'rsi_14', 'rsi_21', 'macd', 'macd_signal',
                        'bb_width_10', 'bb_width_20', 'bb_position_10',
                        'bb_position_20', 'volume_ratio', 'volume_change',
                        'high_low_ratio']

        importances = model_15m.feature_importances_
        indices = np.argsort(importances)[::-1][:5]

        for i in indices:
            print(f"   {feature_names[i]}: {importances[i]:.4f}")

    print("\n3. ê°€ëŠ¥í•œ ì›ì¸:")
    print("   - í›ˆë ¨ ë°ì´í„°ì˜ í´ëž˜ìŠ¤ ë¶ˆê· í˜•")
    print("   - ìž„ê³„ê°’ ì„¤ì • ë¬¸ì œ")
    print("   - íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ ë¬¸ì œ")
    print("   - ê³¼ì í•©/ê³¼ì†Œì í•©")

    print("\n4. í•´ê²° ë°©ë²•:")
    print("   - í´ëž˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¡°ì •")
    print("   - ìž„ê³„ê°’ ìµœì í™”")
    print("   - ë” ë‹¤ì–‘í•œ ì‹œìž¥ ìƒí™© ë°ì´í„°ë¡œ ìž¬í›ˆë ¨")
    print("   - ì•™ìƒë¸” ë°©ë²• ê°œì„ ")

if __name__ == "__main__":
    # ëª¨ë¸ ë¡œë“œ
    models, scalers = load_models()

    # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    results = test_predictions(models, scalers)

    # ë¬¸ì œ ì§„ë‹¨
    diagnose_neutral_bias()

    print("\n" + "="*60)
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    print("="*60)