#!/usr/bin/env python3
"""
íƒ€ì„í”„ë ˆì„ë³„ ë…ë¦½ì ì¸ ë°©í–¥ì„± ì˜ˆì¸¡ ëª¨ë¸
- NEUTRAL ì œê±°, UP/DOWNë§Œ ì˜ˆì¸¡
- ê° íƒ€ì„í”„ë ˆì„ì— ìµœì í™”ëœ ì „ëµ
"""

import ccxt
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import joblib
from datetime import datetime
from loguru import logger
import warnings
warnings.filterwarnings('ignore')

class DirectionalTradingModels:
    def __init__(self):
        self.exchange = ccxt.binance()

        # íƒ€ì„í”„ë ˆì„ë³„ ì „ëµ ì„¤ì •
        self.strategies = {
            '15m': {
                'type': 'scalping',
                'lookback': 20,  # ìµœê·¼ 20ê°œ ìº”ë“¤ ì°¸ì¡°
                'threshold': 0.0005,  # 0.05% (ìŠ¤ìº˜í•‘ìš© ì‘ì€ ì›€ì§ì„)
                'data_limit': 4000,
                'features': ['momentum', 'rsi', 'volume_burst', 'micro_pattern']
            },
            '30m': {
                'type': 'swing',
                'lookback': 30,
                'threshold': 0.001,  # 0.1%
                'data_limit': 3000,
                'features': ['trend', 'macd', 'bollinger', 'volume_trend']
            },
            '4h': {
                'type': 'position',
                'lookback': 50,
                'threshold': 0.002,  # 0.2%
                'data_limit': 2000,
                'features': ['ma_cross', 'trend_strength', 'support_resistance', 'volume_profile']
            },
            '1d': {
                'type': 'trend',
                'lookback': 100,
                'threshold': 0.003,  # 0.3%
                'data_limit': 1000,
                'features': ['long_trend', 'market_structure', 'momentum_divergence', 'accumulation']
            }
        }

    def get_data(self, timeframe):
        """ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        limit = self.strategies[timeframe]['data_limit']
        logger.info(f"  ğŸ“Š ë°ì´í„° ìˆ˜ì§‘: {timeframe} ({limit}ê°œ ìº”ë“¤)")

        ohlcv = self.exchange.fetch_ohlcv('BTC/USDT', timeframe=timeframe, limit=limit)
        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)

        return df

    def create_directional_labels(self, df, timeframe):
        """ë°©í–¥ì„± ë¼ë²¨ ìƒì„± (UP=1, DOWN=0)"""
        strategy = self.strategies[timeframe]
        threshold = strategy['threshold']

        # ë¯¸ë˜ ìˆ˜ìµë¥ 
        future_return = df['close'].shift(-1) / df['close'] - 1

        # ì´ì§„ ë¶„ë¥˜: UP(1) or DOWN(0)
        labels = (future_return > threshold).astype(int)

        # ë¶„í¬ í™•ì¸
        up_count = labels.sum()
        down_count = len(labels) - up_count
        total = len(labels)

        logger.info(f"  ğŸ“ˆ ë¼ë²¨ ë¶„í¬ ({timeframe}):")
        logger.info(f"    UP:   {up_count:4d}ê°œ ({up_count/total*100:5.1f}%)")
        logger.info(f"    DOWN: {down_count:4d}ê°œ ({down_count/total*100:5.1f}%)")

        return labels

    def create_15m_features(self, df):
        """15ë¶„ ìŠ¤ìº˜í•‘ ì „ëµ íŠ¹ì§•"""
        features = pd.DataFrame(index=df.index)

        # 1. ë‹¨ê¸° ëª¨ë©˜í…€
        features['momentum_1'] = df['close'].pct_change(1) * 100
        features['momentum_3'] = df['close'].pct_change(3) * 100
        features['momentum_5'] = df['close'].pct_change(5) * 100

        # 2. RSI (ë¹ ë¥¸ ë°˜ì‘)
        for period in [7, 14]:
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss.replace(0, 1)
            features[f'rsi_{period}'] = 100 - (100 / (1 + rs))

        # 3. ë³¼ë¥¨ ë²„ìŠ¤íŠ¸
        features['volume_burst'] = df['volume'] / df['volume'].rolling(5).mean()
        features['volume_spike'] = (df['volume'] > df['volume'].rolling(20).mean() * 1.5).astype(int)

        # 4. ë¯¸ì„¸ íŒ¨í„´
        features['higher_high'] = (df['high'] > df['high'].shift(1)).astype(int)
        features['higher_low'] = (df['low'] > df['low'].shift(1)).astype(int)
        features['candle_body'] = abs(df['close'] - df['open']) / df['close'] * 100
        features['upper_shadow'] = (df['high'] - df[['close', 'open']].max(axis=1)) / df['close'] * 100

        # 5. ë‹¨ê¸° ë³¼ë¦°ì € ë°´ë“œ
        sma = df['close'].rolling(10).mean()
        std = df['close'].rolling(10).std()
        features['bb_position'] = (df['close'] - sma) / (std * 2)

        return features.fillna(0)

    def create_30m_features(self, df):
        """30ë¶„ ìŠ¤ìœ™ íŠ¸ë ˆì´ë”© íŠ¹ì§•"""
        features = pd.DataFrame(index=df.index)

        # 1. íŠ¸ë Œë“œ ì§€í‘œ
        features['trend_5'] = (df['close'] / df['close'].shift(5) - 1) * 100
        features['trend_10'] = (df['close'] / df['close'].shift(10) - 1) * 100
        features['trend_20'] = (df['close'] / df['close'].shift(20) - 1) * 100

        # 2. MACD
        exp1 = df['close'].ewm(span=12, adjust=False).mean()
        exp2 = df['close'].ewm(span=26, adjust=False).mean()
        features['macd'] = exp1 - exp2
        features['macd_signal'] = features['macd'].ewm(span=9, adjust=False).mean()
        features['macd_histogram'] = features['macd'] - features['macd_signal']

        # 3. ë³¼ë¦°ì € ë°´ë“œ (í‘œì¤€)
        for period in [20, 30]:
            sma = df['close'].rolling(period).mean()
            std = df['close'].rolling(period).std()
            features[f'bb_width_{period}'] = (std * 2) / sma * 100
            features[f'bb_position_{period}'] = (df['close'] - sma) / (std * 2)

        # 4. ë³¼ë¥¨ íŠ¸ë Œë“œ
        features['volume_ma_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
        features['obv'] = (df['volume'] * np.sign(df['close'].diff())).cumsum()
        features['obv_ma'] = features['obv'] / features['obv'].rolling(10).mean()

        return features.fillna(0)

    def create_4h_features(self, df):
        """4ì‹œê°„ í¬ì§€ì…˜ íŠ¸ë ˆì´ë”© íŠ¹ì§•"""
        features = pd.DataFrame(index=df.index)

        # 1. ì´ë™í‰ê·  í¬ë¡œìŠ¤
        ma20 = df['close'].rolling(20).mean()
        ma50 = df['close'].rolling(50).mean()
        ma100 = df['close'].rolling(100).mean()

        features['ma20_ratio'] = df['close'] / ma20
        features['ma50_ratio'] = df['close'] / ma50
        features['ma_cross_20_50'] = (ma20 > ma50).astype(int)
        features['ma_cross_50_100'] = (ma50 > ma100).astype(int)

        # 2. íŠ¸ë Œë“œ ê°•ë„
        features['trend_strength'] = abs(df['close'].pct_change(20)) * 100
        features['trend_consistency'] = (df['close'].diff().rolling(10).apply(lambda x: (x > 0).sum() / len(x)))

        # 3. ì§€ì§€/ì €í•­
        features['distance_from_high'] = (df['high'].rolling(50).max() - df['close']) / df['close'] * 100
        features['distance_from_low'] = (df['close'] - df['low'].rolling(50).min()) / df['close'] * 100

        # 4. ë³¼ë¥¨ í”„ë¡œíŒŒì¼
        features['volume_profile'] = df['volume'].rolling(20).mean() / df['volume'].rolling(100).mean()
        features['volume_trend'] = df['volume'].rolling(20).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0])

        # 5. ATR (ë³€ë™ì„±)
        high_low = df['high'] - df['low']
        high_close = abs(df['high'] - df['close'].shift())
        low_close = abs(df['low'] - df['close'].shift())
        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        features['atr'] = true_range.rolling(14).mean() / df['close'] * 100

        return features.fillna(0)

    def create_1d_features(self, df):
        """1ì¼ íŠ¸ë Œë“œ íŒ”ë¡œì‰ íŠ¹ì§•"""
        features = pd.DataFrame(index=df.index)

        # 1. ì¥ê¸° íŠ¸ë Œë“œ
        ma50 = df['close'].rolling(50).mean()
        ma100 = df['close'].rolling(100).mean()
        ma200 = df['close'].rolling(200).mean()

        features['long_trend'] = (df['close'] / ma200 - 1) * 100
        features['trend_alignment'] = ((df['close'] > ma50) & (ma50 > ma100) & (ma100 > ma200)).astype(int)

        # 2. ì‹œì¥ êµ¬ì¡°
        features['higher_high_weekly'] = (df['high'].rolling(7).max() > df['high'].rolling(7).max().shift(7)).astype(int)
        features['higher_low_weekly'] = (df['low'].rolling(7).min() > df['low'].rolling(7).min().shift(7)).astype(int)

        # 3. ëª¨ë©˜í…€ ë‹¤ì´ë²„ì „ìŠ¤
        rsi_14 = 100 - (100 / (1 + (df['close'].diff().where(lambda x: x > 0, 0).rolling(14).mean() /
                                   (-df['close'].diff().where(lambda x: x < 0, 0).rolling(14).mean()))))
        features['rsi_divergence'] = (df['close'].pct_change(14) * 100) - (rsi_14.pct_change(14) * 100)

        # 4. ì¶•ì /ë¶„ë°°
        features['accumulation'] = ((df['close'] - df['low']) - (df['high'] - df['close'])) / (df['high'] - df['low']) * df['volume']
        features['accumulation_ma'] = features['accumulation'].rolling(20).mean()

        # 5. ì¥ê¸° ë³€ë™ì„±
        features['volatility_30d'] = df['close'].pct_change().rolling(30).std() * 100
        features['volatility_ratio'] = features['volatility_30d'] / df['close'].pct_change().rolling(90).std() / 100

        return features.fillna(0)

    def train_model(self, timeframe):
        """íƒ€ì„í”„ë ˆì„ë³„ ë§ì¶¤ ëª¨ë¸ í›ˆë ¨"""
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ¯ {timeframe} ë°©í–¥ì„± ëª¨ë¸ í›ˆë ¨")
        logger.info(f"   ì „ëµ: {self.strategies[timeframe]['type'].upper()}")
        logger.info(f"{'='*70}")

        # ë°ì´í„° ìˆ˜ì§‘
        df = self.get_data(timeframe)

        # íŠ¹ì§• ìƒì„± (íƒ€ì„í”„ë ˆì„ë³„)
        if timeframe == '15m':
            features = self.create_15m_features(df)
        elif timeframe == '30m':
            features = self.create_30m_features(df)
        elif timeframe == '4h':
            features = self.create_4h_features(df)
        else:  # 1d
            features = self.create_1d_features(df)

        # ë¼ë²¨ ìƒì„±
        labels = self.create_directional_labels(df, timeframe)

        # ì •ë ¬
        X = features.dropna()
        y = labels[X.index][:-1]
        X = X[:-1]

        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]

        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜
        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
        class_weight_dict = dict(zip(np.unique(y_train), class_weights))

        # ìŠ¤ì¼€ì¼ë§
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # ëª¨ë¸ ìƒì„± (íƒ€ì„í”„ë ˆì„ë³„ ìµœì í™”)
        if timeframe in ['15m', '30m']:
            # ë‹¨ê¸°: ë¹ ë¥¸ ë°˜ì‘ ì¤‘ìš”
            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=20,
                class_weight=class_weight_dict,
                random_state=42,
                n_jobs=-1
            )
        else:
            # ì¥ê¸°: ì•ˆì •ì„± ì¤‘ìš”
            model = GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                min_samples_split=50,
                random_state=42
            )

        # í›ˆë ¨
        logger.info("  ğŸ”§ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        model.fit(X_train_scaled, y_train)

        # í‰ê°€
        y_pred = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)

        # ì˜ˆì¸¡ í™•ë¥ 
        y_pred_proba = model.predict_proba(X_test_scaled)

        # ë†’ì€ ì‹ ë¢°ë„ ì˜ˆì¸¡ë§Œ
        high_conf_threshold = 0.65 if timeframe in ['15m', '30m'] else 0.6
        high_conf_mask = np.max(y_pred_proba, axis=1) >= high_conf_threshold

        logger.info(f"\n  ğŸ“Š ì„±ëŠ¥ ì§€í‘œ:")
        logger.info(f"    ì „ì²´ ì •í™•ë„: {accuracy*100:.1f}%")

        if high_conf_mask.any():
            high_conf_acc = accuracy_score(y_test[high_conf_mask], y_pred[high_conf_mask])
            high_conf_ratio = high_conf_mask.sum() / len(y_test) * 100

            logger.info(f"    ê³ ì‹ ë¢°ë„ ì •í™•ë„: {high_conf_acc*100:.1f}% ({high_conf_ratio:.1f}% ì‹ í˜¸)")

            # ê³ ì‹ ë¢°ë„ í˜¼ë™ í–‰ë ¬
            cm = confusion_matrix(y_test[high_conf_mask], y_pred[high_conf_mask])
            if cm.shape == (2, 2):
                logger.info(f"    ê³ ì‹ ë¢°ë„ UP ì •í™•ë„: {cm[1,1]/(cm[1,0]+cm[1,1])*100:.1f}%")
                logger.info(f"    ê³ ì‹ ë¢°ë„ DOWN ì •í™•ë„: {cm[0,0]/(cm[0,0]+cm[0,1])*100:.1f}%")

        # ìµœê·¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
        logger.info(f"\n  ğŸ”® ìµœê·¼ 10ê°œ ì˜ˆì¸¡:")
        recent_features = features.iloc[-11:-1]
        recent_scaled = scaler.transform(recent_features)
        recent_pred = model.predict(recent_scaled)
        recent_proba = model.predict_proba(recent_scaled)

        for i in range(len(recent_pred)):
            direction = "UPğŸ”º" if recent_pred[i] == 1 else "DOWNğŸ”»"
            confidence = max(recent_proba[i]) * 100
            timestamp = recent_features.index[i].strftime('%m-%d %H:%M')
            logger.info(f"    {timestamp}: {direction} ({confidence:.1f}%)")

        # ëª¨ë¸ ì €ì¥
        model_file = f'models/directional_{timeframe}_model.pkl'
        scaler_file = f'models/directional_{timeframe}_scaler.pkl'

        joblib.dump(model, model_file)
        joblib.dump(scaler, scaler_file)

        logger.success(f"  âœ… ëª¨ë¸ ì €ì¥: {model_file}")

        return accuracy, high_conf_acc if high_conf_mask.any() else 0

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    trainer = DirectionalTradingModels()

    logger.info("="*80)
    logger.info("ğŸ¯ íƒ€ì„í”„ë ˆì„ë³„ ë°©í–¥ì„± ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨")
    logger.info("="*80)
    logger.info("")
    logger.info("ì „ëµ:")
    logger.info("  15ë¶„: ìŠ¤ìº˜í•‘ (ë‹¨ê¸° ëª¨ë©˜í…€)")
    logger.info("  30ë¶„: ìŠ¤ìœ™ íŠ¸ë ˆì´ë”© (ì¤‘ê¸° íŠ¸ë Œë“œ)")
    logger.info("  4ì‹œê°„: í¬ì§€ì…˜ íŠ¸ë ˆì´ë”© (ì§€ì§€/ì €í•­)")
    logger.info("  1ì¼: íŠ¸ë Œë“œ íŒ”ë¡œì‰ (ì¥ê¸° ë°©í–¥)")
    logger.info("")

    results = {}

    # ê° íƒ€ì„í”„ë ˆì„ í›ˆë ¨
    for timeframe in ['15m', '30m', '4h', '1d']:
        accuracy, high_conf_acc = trainer.train_model(timeframe)
        results[timeframe] = {
            'accuracy': accuracy,
            'high_conf': high_conf_acc
        }

    # ìµœì¢… ê²°ê³¼
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
    logger.info("="*80)

    for timeframe, result in results.items():
        strategy = trainer.strategies[timeframe]['type']
        logger.info(f"  {timeframe:4s} ({strategy:10s}): {result['accuracy']*100:5.1f}% | ê³ ì‹ ë¢°ë„: {result['high_conf']*100:5.1f}%")

    logger.info("")
    logger.success("âœ… ëª¨ë“  ë°©í–¥ì„± ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
    logger.info("ê° íƒ€ì„í”„ë ˆì„ë³„ë¡œ ë…ë¦½ì ì¸ ê±°ë˜ ì‹ í˜¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()