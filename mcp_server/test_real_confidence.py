#!/usr/bin/env python3
"""
Test script for real confidence calculation
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from btc_predictor import BTCPredictor
import json

def test_real_confidence():
    """Test real confidence calculation (prediction probability Ã— model accuracy)"""
    predictor = BTCPredictor()

    print("=" * 80)
    print("ðŸ§® REAL CONFIDENCE CALCULATION TEST")
    print("Real Confidence = Prediction Probability Ã— Model Accuracy")
    print("=" * 80)

    # Test individual predictions
    print("\nðŸ“Š Individual Model Analysis:")
    print("-" * 80)
    print(f"{'Model':<15} {'Prediction':<12} {'Model Acc':<12} {'Real Conf':<12} {'Interpretation'}")
    print("-" * 80)

    models = [
        ('1h', 'UP'),
        ('1h', 'DOWN'),
        ('4h', 'UP'),
        ('4h', 'DOWN'),
        ('30m', 'UP'),
        ('30m', 'DOWN'),
        ('15m', 'UP'),
    ]

    for timeframe, direction in models:
        pred = predictor.predict(timeframe, direction)

        if 'error' not in pred:
            confidence = pred['confidence']
            accuracy = pred['model_accuracy']
            real_conf = pred['real_confidence']

            # Interpretation
            if real_conf > 0.6:
                interpretation = "âœ… Strong (>60%)"
            elif real_conf > 0.5:
                interpretation = "ðŸŸ¡ Moderate (50-60%)"
            elif real_conf > 0.4:
                interpretation = "ðŸŸ  Weak (40-50%)"
            else:
                interpretation = "âšª Very Weak (<40%)"

            print(f"{timeframe:>2} {direction:<12} {confidence:.1%} Ã— {accuracy:.1f}% = {real_conf:.1%}    {interpretation}")

    # Test consensus
    print("\n" + "=" * 80)
    print("ðŸ“Š CONSENSUS ANALYSIS")
    print("=" * 80)

    consensus = predictor.get_consensus()

    print(f"\nðŸŽ¯ Consensus Direction: {consensus['consensus']}")
    print(f"\nPrediction-based scores:")
    print(f"  UP Score: {consensus['up_score']:.1%}")
    print(f"  DOWN Score: {consensus['down_score']:.1%}")
    print(f"  Confidence: {consensus['confidence']:.1%}")

    print(f"\nâ­ Real confidence scores (accounting for model accuracy):")
    print(f"  Real UP Score: {consensus['real_up_score']:.1%}")
    print(f"  Real DOWN Score: {consensus['real_down_score']:.1%}")
    print(f"  Real Consensus Confidence: {consensus['real_confidence']:.1%}")

    print(f"\nðŸ“ˆ Active Signals:")
    for signal in consensus['active_signals']:
        print(f"  â€¢ {signal}")

    # Analysis
    print("\n" + "=" * 80)
    print("ðŸ’¡ INSIGHT ANALYSIS")
    print("=" * 80)

    print("\nðŸ” What Real Confidence Tells Us:")
    print("â€¢ Real confidence combines current prediction strength with historical accuracy")
    print("â€¢ It represents the actual probability that this prediction will be correct")
    print("â€¢ Values above 60% are considered strong signals for trading")
    print("â€¢ Values below 40% should be treated with caution")

    print("\nðŸ“Š Current Market Situation:")
    real_conf = consensus['real_confidence']
    if real_conf > 0.6:
        print(f"âœ… Strong signal with {real_conf:.1%} actual success probability")
    elif real_conf > 0.5:
        print(f"ðŸŸ¡ Moderate signal with {real_conf:.1%} actual success probability")
    elif real_conf > 0.4:
        print(f"ðŸŸ  Weak signal with {real_conf:.1%} actual success probability")
    else:
        print(f"âšª No clear signal with only {real_conf:.1%} success probability")

    print("\n" + "=" * 80)
    print("âœ… TEST COMPLETE")
    print("=" * 80)


if __name__ == "__main__":
    test_real_confidence()