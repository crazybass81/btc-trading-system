#!/usr/bin/env python3
"""
Comprehensive Backtest for All BTC Direction Prediction Models V2
ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì•™ìƒë¸” ëª¨ë¸ì„ ì²˜ë¦¬í•˜ëŠ” ê°œì„ ëœ ë²„ì „
"""

import numpy as np
import pandas as pd
import joblib
import ccxt
from datetime import datetime, timedelta
import warnings
import json
import os
from sklearn.preprocessing import RobustScaler
warnings.filterwarnings('ignore')

# Exchange ì„¤ì •
exchange = ccxt.binance({
    'enableRateLimit': True,
    'options': {'defaultType': 'future'}
})

def collect_extended_data(symbol='BTC/USDT', timeframe='15m', days=120):
    """ë” ê¸´ ê¸°ê°„ì˜ ë°ì´í„° ìˆ˜ì§‘ (ê¸°ë³¸ 120ì¼)"""
    print(f"\nğŸ“Š Collecting {days} days of {timeframe} data...")

    limit = 1500
    ohlcv_list = []

    # timeframeì„ ë°€ë¦¬ì´ˆë¡œ ë³€í™˜
    timeframe_ms = {
        '15m': 15 * 60 * 1000,
        '30m': 30 * 60 * 1000,
        '1h': 60 * 60 * 1000,
        '4h': 4 * 60 * 60 * 1000
    }[timeframe]

    # ì‹œì‘ ì‹œê°„ ê³„ì‚°
    end_time = exchange.milliseconds()
    start_time = end_time - (days * 24 * 60 * 60 * 1000)
    current_time = start_time

    while current_time < end_time:
        try:
            ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since=current_time, limit=limit)
            if not ohlcv:
                break
            ohlcv_list.extend(ohlcv)
            current_time = ohlcv[-1][0] + timeframe_ms
            if len(ohlcv) < limit:
                break
        except Exception as e:
            print(f"Error fetching data: {e}")
            break

    # DataFrame ìƒì„±
    df = pd.DataFrame(ohlcv_list, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
    df = df.drop_duplicates(subset=['timestamp']).sort_values('timestamp').reset_index(drop=True)

    print(f"âœ… Collected {len(df)} candles from {df['timestamp'].min()} to {df['timestamp'].max()}")
    return df

def prepare_features(df):
    """íŠ¹ì§• ìƒì„±"""
    # ê¸°ë³¸ íŠ¹ì§•
    df['returns'] = df['close'].pct_change()
    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
    df['volume_ratio'] = df['volume'] / df['volume'].rolling(window=20).mean()

    # ë³€ë™ì„±
    df['volatility'] = df['returns'].rolling(window=20).std()
    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']
    df['close_open_ratio'] = (df['close'] - df['open']) / df['open']

    # ê¸°ìˆ ì  ì§€í‘œ
    for period in [5, 10, 20, 50]:
        df[f'sma_{period}'] = df['close'].rolling(window=period).mean()
        df[f'sma_ratio_{period}'] = df['close'] / df[f'sma_{period}']

    for period in [7, 14, 21]:
        df[f'rsi_{period}'] = calculate_rsi(df['close'], period)

    # MACD
    exp1 = df['close'].ewm(span=12, adjust=False).mean()
    exp2 = df['close'].ewm(span=26, adjust=False).mean()
    df['macd'] = exp1 - exp2
    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()
    df['macd_diff'] = df['macd'] - df['macd_signal']

    # ë³¼ë¦°ì € ë°´ë“œ
    for period in [20, 30]:
        sma = df['close'].rolling(window=period).mean()
        std = df['close'].rolling(window=period).std()
        df[f'bb_upper_{period}'] = sma + (std * 2)
        df[f'bb_lower_{period}'] = sma - (std * 2)
        df[f'bb_ratio_{period}'] = (df['close'] - df[f'bb_lower_{period}']) / (df[f'bb_upper_{period}'] - df[f'bb_lower_{period}'])

    # ì‹œê°„ íŠ¹ì§•
    df['hour'] = df['timestamp'].dt.hour
    df['dayofweek'] = df['timestamp'].dt.dayofweek
    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['day_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
    df['day_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)

    return df

def calculate_rsi(prices, period=14):
    """RSI ê³„ì‚°"""
    delta = prices.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

def predict_ensemble(model_data, X_test):
    """ì•™ìƒë¸” ëª¨ë¸ ì˜ˆì¸¡ í•¨ìˆ˜"""
    models = model_data['models']
    weights = model_data.get('weights', [1/len(models)] * len(models))
    scaler = model_data.get('scaler')

    # íŠ¹ì§• ìŠ¤ì¼€ì¼ë§
    if scaler:
        X_test_scaled = scaler.transform(X_test)
    else:
        X_test_scaled = X_test

    # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘
    predictions = []
    for model in models:
        try:
            pred = model.predict_proba(X_test_scaled)[:, 1]
            predictions.append(pred)
        except:
            pred = model.predict(X_test_scaled)
            predictions.append(pred)

    # ê°€ì¤‘ í‰ê· 
    weighted_predictions = np.average(predictions, axis=0, weights=weights)

    # ì´ì§„ ë¶„ë¥˜ (0.5 ê¸°ì¤€)
    return (weighted_predictions > 0.5).astype(int)

def backtest_model(model_path, df, timeframe, direction, days_for_test=90):
    """ë‹¨ì¼ ëª¨ë¸ ë°±í…ŒìŠ¤íŠ¸"""
    model_name = os.path.basename(model_path).replace('.pkl', '')
    print(f"\nğŸ” Backtesting {model_name}...")

    try:
        # ëª¨ë¸ ë¡œë“œ
        model_data = joblib.load(model_path)

        # íƒ€ê²Ÿ ìƒì„± (ë‹¤ìŒ ë´‰ ë°©í–¥)
        df['next_direction'] = (df['close'].shift(-1) > df['close']).astype(int)

        # íŠ¹ì§• ì¤€ë¹„
        feature_cols = [col for col in df.columns if col not in
                       ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'next_direction']]

        # NaN ì œê±°
        df_clean = df.dropna()

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ (ë§ˆì§€ë§‰ 90ì¼)
        split_date = df_clean['timestamp'].max() - timedelta(days=days_for_test)
        test_df = df_clean[df_clean['timestamp'] > split_date].copy()

        if len(test_df) < 100:
            print(f"âŒ Insufficient test data for {model_name}")
            return None

        X_test = test_df[feature_cols]
        y_test = test_df['next_direction']

        # ì•™ìƒë¸” ì˜ˆì¸¡
        if isinstance(model_data, dict):
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì•™ìƒë¸” ëª¨ë¸
            y_pred = predict_ensemble(model_data, X_test)
        else:
            # ë‹¨ì¼ ëª¨ë¸
            y_pred = model_data.predict(X_test)

        # ì •í™•ë„ ê³„ì‚°
        if direction.lower() == 'up':
            # UP ëª¨ë¸: ìƒìŠ¹ ì˜ˆì¸¡
            predictions_correct = (y_pred == 1) & (y_test == 1)  # ìƒìŠ¹ ì˜ˆì¸¡ì´ ë§ì€ ê²½ìš°
            predictions_made = (y_pred == 1)  # ìƒìŠ¹ ì˜ˆì¸¡í•œ ëª¨ë“  ê²½ìš°
        else:
            # DOWN ëª¨ë¸: í•˜ë½ ì˜ˆì¸¡
            predictions_correct = (y_pred == 0) & (y_test == 0)  # í•˜ë½ ì˜ˆì¸¡ì´ ë§ì€ ê²½ìš°
            predictions_made = (y_pred == 0)  # í•˜ë½ ì˜ˆì¸¡í•œ ëª¨ë“  ê²½ìš°

        # ì •í™•ë„ ê³„ì‚°
        if predictions_made.sum() > 0:
            accuracy = predictions_correct.sum() / predictions_made.sum()
        else:
            accuracy = 0

        # ìƒì„¸ ê²°ê³¼
        results = {
            'model': model_name,
            'timeframe': timeframe,
            'direction': direction.upper(),
            'test_days': days_for_test,
            'test_samples': len(test_df),
            'predictions_made': int(predictions_made.sum()),
            'correct_predictions': int(predictions_correct.sum()),
            'accuracy': float(accuracy),
            'test_period': f"{test_df['timestamp'].min()} to {test_df['timestamp'].max()}",

            # ì¶”ê°€ í†µê³„
            'daily_predictions': predictions_made.sum() / days_for_test,
            'win_rate': float(accuracy * 100),

            # ì‹œê°„ëŒ€ë³„ ì„±ëŠ¥
            'performance_by_hour': {},
            'performance_by_day': {},

            # ì›”ë³„ ì„±ëŠ¥
            'monthly_performance': {}
        }

        # ì‹œê°„ëŒ€ë³„ ë¶„ì„
        test_df['prediction'] = y_pred
        test_df['correct'] = predictions_correct

        # ì‹œê°„ë³„ ì„±ëŠ¥
        for hour in range(0, 24, 3):  # 3ì‹œê°„ ë‹¨ìœ„ë¡œ
            hour_data = test_df[test_df['hour'].between(hour, hour+2)]
            if len(hour_data) > 10:
                if direction.lower() == 'up':
                    hour_preds = (hour_data['prediction'] == 1).sum()
                    hour_correct = ((hour_data['prediction'] == 1) & (hour_data['next_direction'] == 1)).sum()
                else:
                    hour_preds = (hour_data['prediction'] == 0).sum()
                    hour_correct = ((hour_data['prediction'] == 0) & (hour_data['next_direction'] == 0)).sum()

                if hour_preds > 0:
                    results['performance_by_hour'][f"{hour:02d}:00-{hour+2:02d}:00"] = {
                        'accuracy': float(hour_correct / hour_preds),
                        'predictions': int(hour_preds)
                    }

        # ìš”ì¼ë³„ ë¶„ì„
        days_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
        for day in range(7):
            day_data = test_df[test_df['dayofweek'] == day]
            if len(day_data) > 10:
                if direction.lower() == 'up':
                    day_preds = (day_data['prediction'] == 1).sum()
                    day_correct = ((day_data['prediction'] == 1) & (day_data['next_direction'] == 1)).sum()
                else:
                    day_preds = (day_data['prediction'] == 0).sum()
                    day_correct = ((day_data['prediction'] == 0) & (day_data['next_direction'] == 0)).sum()

                if day_preds > 0:
                    results['performance_by_day'][days_names[day]] = {
                        'accuracy': float(day_correct / day_preds),
                        'predictions': int(day_preds)
                    }

        # ì›”ë³„ ì„±ëŠ¥ ë¶„ì„
        test_df['month'] = test_df['timestamp'].dt.to_period('M')
        for month in test_df['month'].unique():
            month_data = test_df[test_df['month'] == month]
            if len(month_data) > 10:
                if direction.lower() == 'up':
                    month_preds = (month_data['prediction'] == 1).sum()
                    month_correct = ((month_data['prediction'] == 1) & (month_data['next_direction'] == 1)).sum()
                else:
                    month_preds = (month_data['prediction'] == 0).sum()
                    month_correct = ((month_data['prediction'] == 0) & (month_data['next_direction'] == 0)).sum()

                if month_preds > 0:
                    results['monthly_performance'][str(month)] = {
                        'accuracy': float(month_correct / month_preds),
                        'predictions': int(month_preds),
                        'samples': len(month_data)
                    }

        print(f"âœ… {model_name}: {accuracy*100:.1f}% accuracy on {predictions_made.sum()} predictions")
        print(f"   Test period: {test_df['timestamp'].min().date()} to {test_df['timestamp'].max().date()}")

        # ìµœê³ /ìµœì € ì„±ëŠ¥ ì‹œê°„ëŒ€ ì¶œë ¥
        if results['performance_by_hour']:
            best_hour = max(results['performance_by_hour'].items(), key=lambda x: x[1]['accuracy'])
            worst_hour = min(results['performance_by_hour'].items(), key=lambda x: x[1]['accuracy'])
            print(f"   Best time: {best_hour[0]} ({best_hour[1]['accuracy']*100:.1f}%)")
            print(f"   Worst time: {worst_hour[0]} ({worst_hour[1]['accuracy']*100:.1f}%)")

        return results

    except Exception as e:
        print(f"âŒ Error backtesting {model_name}: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 70)
    print("ğŸš€ COMPREHENSIVE MODEL BACKTEST - EXTENDED PERIOD (120 DAYS)")
    print("=" * 70)

    # ëª¨ë¸ ëª©ë¡
    models_to_test = [
        ('models/deep_ensemble_1h_up_model.pkl', '1h', 'up'),
        ('models/deep_ensemble_1h_down_model.pkl', '1h', 'down'),
        ('models/deep_ensemble_4h_up_model.pkl', '4h', 'up'),
        ('models/deep_ensemble_4h_down_model.pkl', '4h', 'down'),
        ('models/deep_ensemble_30m_up_model.pkl', '30m', 'up'),
        ('models/deep_ensemble_30m_down_model.pkl', '30m', 'down'),
        ('models/advanced_15m_up_model.pkl', '15m', 'up'),
        ('models/deep_ensemble_15m_up_model.pkl', '15m', 'up')
    ]

    all_results = []

    # ê° ì‹œê°„ë´‰ë³„ ë°ì´í„° ìˆ˜ì§‘ ë° ë°±í…ŒìŠ¤íŠ¸
    for timeframe in ['15m', '30m', '1h', '4h']:
        print(f"\n{'='*70}")
        print(f"ğŸ“Š Processing {timeframe} timeframe...")
        print(f"{'='*70}")

        # 120ì¼ ë°ì´í„° ìˆ˜ì§‘
        df = collect_extended_data(timeframe=timeframe, days=120)

        # íŠ¹ì§• ìƒì„±
        df = prepare_features(df)

        # í•´ë‹¹ ì‹œê°„ë´‰ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸
        for model_path, tf, direction in models_to_test:
            if tf == timeframe:
                result = backtest_model(model_path, df, timeframe, direction, days_for_test=90)
                if result:
                    all_results.append(result)

    # ê²°ê³¼ ì €ì¥
    if all_results:
        results_df = pd.DataFrame(all_results)
        results_df.to_csv('backtest_results_extended.csv', index=False)

        # JSONìœ¼ë¡œë„ ì €ì¥
        with open('backtest_results_extended.json', 'w') as f:
            json.dump(all_results, f, indent=2, default=str)

        # ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 70)
        print("ğŸ“Š BACKTEST SUMMARY - 90 DAYS TEST PERIOD")
        print("=" * 70)

        for result in all_results:
            print(f"\nğŸ¯ {result['model']}")
            print(f"   Timeframe: {result['timeframe']}")
            print(f"   Direction: {result['direction']}")
            print(f"   Accuracy: {result['accuracy']*100:.2f}%")
            print(f"   Total Predictions: {result['predictions_made']}")
            print(f"   Correct Predictions: {result['correct_predictions']}")
            print(f"   Daily Average: {result['daily_predictions']:.1f} predictions/day")

            # ì›”ë³„ ì„±ëŠ¥
            if result.get('monthly_performance'):
                print(f"   Monthly Performance:")
                for month, perf in sorted(result['monthly_performance'].items()):
                    print(f"      {month}: {perf['accuracy']*100:.1f}% ({perf['predictions']} predictions)")

        # ì „ì²´ í‰ê· 
        avg_accuracy = results_df['accuracy'].mean()
        total_predictions = results_df['predictions_made'].sum()
        total_correct = results_df['correct_predictions'].sum()

        print(f"\n{'='*70}")
        print(f"ğŸ“ˆ OVERALL STATISTICS")
        print(f"{'='*70}")
        print(f"Average Accuracy: {avg_accuracy*100:.2f}%")
        print(f"Total Predictions: {total_predictions}")
        print(f"Total Correct: {total_correct}")
        print(f"Overall Win Rate: {total_correct/total_predictions*100:.2f}%")
        print(f"Test Period: 90 days of real market data")
        print(f"Total Models Tested: {len(all_results)}")
        print(f"{'='*70}")

        # ìµœê³ /ìµœì € ì„±ëŠ¥ ëª¨ë¸
        best_model = results_df.loc[results_df['accuracy'].idxmax()]
        worst_model = results_df.loc[results_df['accuracy'].idxmin()]

        print(f"\nğŸ† Best Model: {best_model['model']}")
        print(f"   Accuracy: {best_model['accuracy']*100:.2f}%")
        print(f"   Direction: {best_model['direction']}")

        print(f"\nâš ï¸ Weakest Model: {worst_model['model']}")
        print(f"   Accuracy: {worst_model['accuracy']*100:.2f}%")
        print(f"   Direction: {worst_model['direction']}")

        return results_df
    else:
        print("\nâŒ No successful backtests completed")
        return None

if __name__ == "__main__":
    results = main()