#!/usr/bin/env python3
"""
ëª¨ë¸ íŽ¸í–¥ ë¬¸ì œ í•´ê²° ìŠ¤í¬ë¦½íŠ¸
- ê· í˜•ìž¡ížŒ ë°ì´í„°ë¡œ ìž¬í›ˆë ¨
- í´ëž˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
- ë‹¤ì–‘í•œ ì‹œìž¥ ìƒí™© í¬í•¨
"""

import ccxt
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from xgboost import XGBClassifier
import joblib
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class BalancedModelTrainer:
    def __init__(self):
        self.exchange = ccxt.binance()

        # ê· í˜•ìž¡ížŒ ìž„ê³„ê°’ ì„¤ì • (ë” ë¯¼ê°í•˜ê²Œ)
        self.thresholds = {
            '15m': 0.0003,  # 0.03% - ë§¤ìš° ë¯¼ê°
            '30m': 0.0005,  # 0.05% - ë¯¼ê°
            '1h': 0.001,    # 0.1% - ì ë‹¹
            '4h': 0.002     # 0.2% - í‘œì¤€
        }

        # ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ (ë” ê¸´ ê¸°ê°„)
        self.data_limits = {
            '15m': 10000,  # ì•½ 4ì¼
            '30m': 8000,   # ì•½ 1ì£¼ì¼
            '1h': 6000,    # ì•½ 8ê°œì›”
            '4h': 3000     # ì•½ 16ê°œì›”
        }

    def get_extended_data(self, timeframe):
        """ë” ê¸´ ê¸°ê°„ì˜ ë°ì´í„° ìˆ˜ì§‘"""
        limit = self.data_limits[timeframe]
        print(f"\nðŸ“Š í™•ìž¥ ë°ì´í„° ìˆ˜ì§‘: {timeframe} ({limit}ê°œ ìº”ë“¤)")

        try:
            ohlcv = self.exchange.fetch_ohlcv('BTC/USDT', timeframe=timeframe, limit=limit)
            df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df.set_index('timestamp', inplace=True)

            # ê¸°ê°„ ì •ë³´
            start_date = df.index[0].strftime('%Y-%m-%d')
            end_date = df.index[-1].strftime('%Y-%m-%d')
            print(f"  ê¸°ê°„: {start_date} ~ {end_date}")

            return df
        except Exception as e:
            print(f"  âš ï¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None

    def create_balanced_labels(self, df, timeframe):
        """ê· í˜•ìž¡ížŒ ë¼ë²¨ ìƒì„±"""
        threshold = self.thresholds[timeframe]

        # ë¯¸ëž˜ ìˆ˜ìµë¥  ê³„ì‚°
        future_return = df['close'].shift(-1) / df['close'] - 1

        # ë™ì  ìž„ê³„ê°’ (ë³€ë™ì„± ê¸°ë°˜)
        volatility = future_return.rolling(window=100).std()
        dynamic_threshold = threshold * (1 + volatility)

        # ì´ì§„ ë¶„ë¥˜ ë¼ë²¨ (0: DOWN, 1: UP)
        labels = (future_return > 0).astype(int)  # ë‹¨ìˆœížˆ ìƒìŠ¹/í•˜ë½ìœ¼ë¡œ êµ¬ë¶„

        # ë¶„í¬ í™•ì¸
        up_count = labels.sum()
        down_count = len(labels) - up_count
        total = len(labels)

        print(f"\nðŸ“ˆ ë¼ë²¨ ë¶„í¬ ({timeframe}):")
        print(f"  UP:   {up_count:5d}ê°œ ({up_count/total*100:5.1f}%)")
        print(f"  DOWN: {down_count:5d}ê°œ ({down_count/total*100:5.1f}%)")

        # ë¶ˆê· í˜• ë¹„ìœ¨
        imbalance_ratio = max(up_count, down_count) / min(up_count, down_count)
        print(f"  ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}:1")

        return labels

    def create_enhanced_features(self, df):
        """í–¥ìƒëœ íŠ¹ì§• ìƒì„±"""
        features = pd.DataFrame(index=df.index)

        # ê°€ê²© ë³€í™”ìœ¨
        for period in [1, 3, 5, 10, 20, 50]:
            features[f'return_{period}'] = df['close'].pct_change(period)

        # ì´ë™í‰ê· 
        for period in [5, 10, 20, 50, 100]:
            ma = df['close'].rolling(window=period).mean()
            features[f'ma_{period}_ratio'] = df['close'] / ma - 1
            features[f'ma_{period}_slope'] = ma.pct_change(5)

        # RSI
        for period in [7, 14, 21]:
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            features[f'rsi_{period}'] = 100 - (100 / (1 + rs))

        # ë³¼ë¦°ì € ë°´ë“œ
        for period in [20, 50]:
            ma = df['close'].rolling(window=period).mean()
            std = df['close'].rolling(window=period).std()
            features[f'bb_{period}_upper'] = (df['close'] - (ma + 2*std)) / df['close']
            features[f'bb_{period}_lower'] = ((ma - 2*std) - df['close']) / df['close']
            features[f'bb_{period}_width'] = 4 * std / ma

        # ê±°ëž˜ëŸ‰ ì§€í‘œ
        features['volume_ratio'] = df['volume'] / df['volume'].rolling(window=20).mean()
        features['volume_trend'] = df['volume'].rolling(window=10).mean().pct_change(5)

        # MACD
        exp1 = df['close'].ewm(span=12, adjust=False).mean()
        exp2 = df['close'].ewm(span=26, adjust=False).mean()
        macd = exp1 - exp2
        signal = macd.ewm(span=9, adjust=False).mean()
        features['macd'] = macd / df['close']
        features['macd_signal'] = signal / df['close']
        features['macd_hist'] = (macd - signal) / df['close']

        # ë³€ë™ì„±
        features['volatility'] = df['close'].pct_change().rolling(window=20).std()
        features['high_low_ratio'] = (df['high'] - df['low']) / df['close']

        # ì‹œê°„ íŠ¹ì§•
        features['hour'] = df.index.hour
        features['day_of_week'] = df.index.dayofweek

        return features

    def train_balanced_model(self, timeframe, model_type='gradientboost'):
        """ê· í˜•ìž¡ížŒ ëª¨ë¸ í›ˆë ¨"""
        print(f"\n{'='*60}")
        print(f"ðŸš€ {timeframe} ê· í˜• ëª¨ë¸ í›ˆë ¨ ì‹œìž‘")
        print(f"{'='*60}")

        # ë°ì´í„° ìˆ˜ì§‘
        df = self.get_extended_data(timeframe)
        if df is None:
            return None

        # ë¼ë²¨ ìƒì„±
        labels = self.create_balanced_labels(df, timeframe)

        # íŠ¹ì§• ìƒì„±
        features = self.create_enhanced_features(df)

        # NaN ì œê±°
        valid_idx = ~(features.isna().any(axis=1) | labels.isna())
        X = features[valid_idx]
        y = labels[valid_idx]

        print(f"\ní›ˆë ¨ ë°ì´í„°: {len(X)}ê°œ ìƒ˜í”Œ")

        # í´ëž˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
        class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
        print(f"í´ëž˜ìŠ¤ ê°€ì¤‘ì¹˜: DOWN={class_weights[0]:.2f}, UP={class_weights[1]:.2f}")

        # ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ ìœ ì§€)
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # ëª¨ë¸ ì„ íƒ ë° í›ˆë ¨
        if model_type == 'gradientboost':
            model = GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=5,
                min_samples_split=20,
                min_samples_leaf=10,
                subsample=0.8,
                random_state=42
            )
            # GradientBoostingì€ sample_weight ì‚¬ìš©
            sample_weights = np.array([class_weight_dict[int(label)] for label in y_train])
            model.fit(X_train_scaled, y_train, sample_weight=sample_weights)

        elif model_type == 'xgboost':
            model = XGBClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=5,
                scale_pos_weight=class_weights[1]/class_weights[0],  # UP/DOWN ë¹„ìœ¨
                random_state=42,
                use_label_encoder=False,
                eval_metric='logloss'
            )
            model.fit(X_train_scaled, y_train)

        elif model_type == 'neuralnet':
            model = MLPClassifier(
                hidden_layer_sizes=(100, 50, 25),
                activation='relu',
                learning_rate_init=0.001,
                max_iter=500,
                random_state=42
            )
            # MLPClassifierëŠ” sample_weight ì‚¬ìš©
            sample_weights = np.array([class_weight_dict[int(label)] for label in y_train])
            model.fit(X_train_scaled, y_train)  # ì¼ë¶€ ëª¨ë¸ì€ sample_weightë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ

        else:  # randomforest
            model = RandomForestClassifier(
                n_estimators=200,
                max_depth=10,
                min_samples_split=20,
                min_samples_leaf=10,
                class_weight=class_weight_dict,
                random_state=42
            )
            model.fit(X_train_scaled, y_train)

        # í‰ê°€
        y_pred_train = model.predict(X_train_scaled)
        y_pred_test = model.predict(X_test_scaled)

        print(f"\nðŸ“Š ëª¨ë¸ í‰ê°€:")
        print(f"í›ˆë ¨ ì •í™•ë„: {accuracy_score(y_train, y_pred_train)*100:.1f}%")
        print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy_score(y_test, y_pred_test)*100:.1f}%")

        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y_test, y_pred_test)
        print(f"\ní˜¼ë™ í–‰ë ¬:")
        print(f"         ì˜ˆì¸¡DOWN  ì˜ˆì¸¡UP")
        print(f"ì‹¤ì œDOWN    {cm[0,0]:4d}    {cm[0,1]:4d}")
        print(f"ì‹¤ì œUP      {cm[1,0]:4d}    {cm[1,1]:4d}")

        # ê° í´ëž˜ìŠ¤ë³„ ì •í™•ë„
        if len(cm) >= 2:
            down_acc = cm[0,0] / (cm[0,0] + cm[0,1]) * 100 if (cm[0,0] + cm[0,1]) > 0 else 0
            up_acc = cm[1,1] / (cm[1,0] + cm[1,1]) * 100 if (cm[1,0] + cm[1,1]) > 0 else 0
            print(f"\nDOWN ì˜ˆì¸¡ ì •í™•ë„: {down_acc:.1f}%")
            print(f"UP ì˜ˆì¸¡ ì •í™•ë„: {up_acc:.1f}%")

        # ì˜ˆì¸¡ ë¶„í¬ í™•ì¸
        pred_dist = pd.Series(y_pred_test).value_counts()
        print(f"\nì˜ˆì¸¡ ë¶„í¬:")
        for val in [0, 1]:
            count = pred_dist.get(val, 0)
            pct = count / len(y_pred_test) * 100
            label = "DOWN" if val == 0 else "UP"
            print(f"  {label}: {count}ê°œ ({pct:.1f}%)")

        # ìµœê·¼ ì˜ˆì¸¡ í™•ë¥  í™•ì¸
        recent_probs = model.predict_proba(X_test_scaled[-10:])
        print(f"\nìµœê·¼ 10ê°œ ì˜ˆì¸¡ í™•ë¥ :")
        for i, (down_prob, up_prob) in enumerate(recent_probs):
            actual = "UP" if y_test.iloc[-10+i] == 1 else "DOWN"
            pred = "UP" if up_prob > 0.5 else "DOWN"
            print(f"  {i+1:2d}: DOWN={down_prob:.1%}, UP={up_prob:.1%} | ì‹¤ì œ={actual}, ì˜ˆì¸¡={pred}")

        # ëª¨ë¸ ì €ìž¥
        model_info = {
            'model': model,
            'scaler': scaler,
            'features': list(features.columns),
            'accuracy': accuracy_score(y_test, y_pred_test),
            'timeframe': timeframe,
            'threshold': self.thresholds[timeframe],
            'trained_at': datetime.now().isoformat()
        }

        # íŒŒì¼ëª…
        filename = f"balanced_{timeframe}_{model_type}_model.pkl"
        filepath = f"models/{filename}"
        joblib.dump(model_info, filepath)
        print(f"\nâœ… ëª¨ë¸ ì €ìž¥: {filepath}")

        return model_info

    def test_realtime_predictions(self, model_info):
        """ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
        timeframe = model_info['timeframe']
        print(f"\nðŸ”® ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ({timeframe})")

        # ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        ohlcv = self.exchange.fetch_ohlcv('BTC/USDT', timeframe=timeframe, limit=200)
        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)

        # íŠ¹ì§• ìƒì„±
        features = self.create_enhanced_features(df)

        # ìµœì‹  ë°ì´í„°ë§Œ ì‚¬ìš©
        latest_features = features.iloc[-1:].dropna(axis=1, how='any')

        # ëª¨ë¸ì— ë§žëŠ” íŠ¹ì§• ì„ íƒ
        model_features = [f for f in model_info['features'] if f in latest_features.columns]
        X = latest_features[model_features]

        if len(X) > 0 and len(model_features) == len(model_info['features']):
            # ì˜ˆì¸¡
            X_scaled = model_info['scaler'].transform(X)
            prediction = model_info['model'].predict(X_scaled)[0]
            proba = model_info['model'].predict_proba(X_scaled)[0]

            # í˜„ìž¬ ê°€ê²©
            current_price = df['close'].iloc[-1]
            price_change = df['close'].pct_change().iloc[-1] * 100

            print(f"í˜„ìž¬ ê°€ê²©: ${current_price:,.0f} ({price_change:+.2f}%)")
            print(f"ì˜ˆì¸¡: {'UP â†‘' if prediction == 1 else 'DOWN â†“'}")
            print(f"í™•ë¥ : DOWN={proba[0]:.1%}, UP={proba[1]:.1%}")
        else:
            print("âš ï¸ íŠ¹ì§• ìƒì„± ì‹¤íŒ¨")

def main():
    trainer = BalancedModelTrainer()

    print("=" * 60)
    print("ðŸ”§ ëª¨ë¸ íŽ¸í–¥ ë¬¸ì œ í•´ê²° ì‹œìž‘")
    print("=" * 60)

    # ê° íƒ€ìž„í”„ë ˆìž„ë³„ë¡œ ê· í˜•ìž¡ížŒ ëª¨ë¸ í›ˆë ¨
    timeframes = ['15m', '30m', '1h', '4h']
    model_types = {
        '15m': 'gradientboost',
        '30m': 'neuralnet',
        '1h': 'gradientboost',
        '4h': 'neuralnet'
    }

    trained_models = {}

    for tf in timeframes:
        model_type = model_types[tf]
        model_info = trainer.train_balanced_model(tf, model_type)
        if model_info:
            trained_models[tf] = model_info
            trainer.test_realtime_predictions(model_info)

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ðŸ“‹ í›ˆë ¨ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    for tf, model_info in trained_models.items():
        print(f"\n{tf}: ì •í™•ë„ {model_info['accuracy']*100:.1f}%")

        # ì‹¤ì œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
        ohlcv = trainer.exchange.fetch_ohlcv('BTC/USDT', timeframe=tf, limit=100)
        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)

        features = trainer.create_enhanced_features(df).iloc[-10:]
        valid_features = features.dropna()

        if len(valid_features) > 0:
            model_features = [f for f in model_info['features'] if f in valid_features.columns]
            if len(model_features) == len(model_info['features']):
                X = valid_features[model_features]
                X_scaled = model_info['scaler'].transform(X)
                predictions = model_info['model'].predict(X_scaled)

                up_count = (predictions == 1).sum()
                down_count = (predictions == 0).sum()
                print(f"  ìµœê·¼ {len(predictions)}ê°œ ì˜ˆì¸¡: UP={up_count}, DOWN={down_count}")

if __name__ == "__main__":
    main()