#!/usr/bin/env python3
"""
ê°œì„ ëœ ML ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
- ë” ë‚˜ì€ íŠ¹ì§• ê³µí•™
- ì ì‘ì  ì„ê³„ê°’
- ì•™ìƒë¸” ë°©ë²•
- êµì°¨ ê²€ì¦
"""

import ccxt
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from xgboost import XGBClassifier
import joblib
from datetime import datetime
import talib
import warnings
warnings.filterwarnings('ignore')

class ImprovedModelTrainer:
    def __init__(self):
        self.exchange = ccxt.binance()

    def get_data(self, timeframe, limit=10000):
        """ë°ì´í„° ìˆ˜ì§‘"""
        print(f"ğŸ“Š ë°ì´í„° ìˆ˜ì§‘: {timeframe} ({limit}ê°œ ìº”ë“¤)")

        ohlcv = self.exchange.fetch_ohlcv('BTC/USDT', timeframe=timeframe, limit=limit)
        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)

        return df

    def create_advanced_features(self, df):
        """ê³ ê¸‰ íŠ¹ì§• ìƒì„± (TA-Lib í™œìš©)"""
        features = pd.DataFrame(index=df.index)

        # ê°€ê²© ë°ì´í„° numpy ë°°ì—´ë¡œ ë³€í™˜
        open_price = df['open'].values
        high = df['high'].values
        low = df['low'].values
        close = df['close'].values
        volume = df['volume'].values

        # 1. ëª¨ë©˜í…€ ì§€í‘œ
        features['rsi'] = talib.RSI(close, timeperiod=14)
        features['rsi_fast'] = talib.RSI(close, timeperiod=7)
        features['rsi_slow'] = talib.RSI(close, timeperiod=21)

        # 2. ì´ë™í‰ê· 
        features['sma_10'] = talib.SMA(close, timeperiod=10)
        features['sma_20'] = talib.SMA(close, timeperiod=20)
        features['sma_50'] = talib.SMA(close, timeperiod=50)
        features['ema_12'] = talib.EMA(close, timeperiod=12)
        features['ema_26'] = talib.EMA(close, timeperiod=26)

        # ì´ë™í‰ê·  ë¹„ìœ¨
        features['price_sma10_ratio'] = close / features['sma_10']
        features['price_sma20_ratio'] = close / features['sma_20']
        features['sma10_sma20_ratio'] = features['sma_10'] / features['sma_20']

        # 3. MACD
        macd, macd_signal, macd_hist = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)
        features['macd'] = macd
        features['macd_signal'] = macd_signal
        features['macd_hist'] = macd_hist

        # 4. ë³¼ë¦°ì € ë°´ë“œ
        bb_upper, bb_middle, bb_lower = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)
        features['bb_upper'] = bb_upper
        features['bb_middle'] = bb_middle
        features['bb_lower'] = bb_lower
        features['bb_width'] = bb_upper - bb_lower
        features['bb_position'] = (close - bb_lower) / (bb_upper - bb_lower)

        # 5. ìŠ¤í† ìºìŠ¤í‹±
        slowk, slowd = talib.STOCH(high, low, close, fastk_period=14, slowk_period=3, slowd_period=3)
        features['stoch_k'] = slowk
        features['stoch_d'] = slowd

        # 6. ADX (íŠ¸ë Œë“œ ê°•ë„)
        features['adx'] = talib.ADX(high, low, close, timeperiod=14)
        features['plus_di'] = talib.PLUS_DI(high, low, close, timeperiod=14)
        features['minus_di'] = talib.MINUS_DI(high, low, close, timeperiod=14)

        # 7. ATR (ë³€ë™ì„±)
        features['atr'] = talib.ATR(high, low, close, timeperiod=14)
        features['atr_ratio'] = features['atr'] / close

        # 8. ë³¼ë¥¨ ì§€í‘œ
        features['obv'] = talib.OBV(close, volume)
        features['ad'] = talib.AD(high, low, close, volume)
        features['adosc'] = talib.ADOSC(high, low, close, volume, fastperiod=3, slowperiod=10)

        # 9. íŒ¨í„´ ì¸ì‹ (ìº”ë“¤ íŒ¨í„´)
        features['hammer'] = talib.CDLHAMMER(open_price, high, low, close)
        features['doji'] = talib.CDLDOJI(open_price, high, low, close)
        features['engulfing'] = talib.CDLENGULFING(open_price, high, low, close)
        features['morning_star'] = talib.CDLMORNINGSTAR(open_price, high, low, close)
        features['three_white_soldiers'] = talib.CDL3WHITESOLDIERS(open_price, high, low, close)

        # 10. ì¶”ê°€ ì§€í‘œ
        features['cci'] = talib.CCI(high, low, close, timeperiod=14)
        features['mfi'] = talib.MFI(high, low, close, volume, timeperiod=14)
        features['willr'] = talib.WILLR(high, low, close, timeperiod=14)
        features['roc'] = talib.ROC(close, timeperiod=10)
        features['mom'] = talib.MOM(close, timeperiod=10)

        # 11. ê°€ê²© ë³€í™”ìœ¨
        features['return_1'] = df['close'].pct_change(1)
        features['return_3'] = df['close'].pct_change(3)
        features['return_5'] = df['close'].pct_change(5)
        features['return_10'] = df['close'].pct_change(10)

        # 12. ê³ /ì € ë¹„ìœ¨
        features['high_low_ratio'] = (high - low) / close
        features['close_open_ratio'] = (close - open_price) / open_price

        # 13. ì‹œê°„ íŠ¹ì§•
        features['hour'] = df.index.hour
        features['day_of_week'] = df.index.dayofweek

        # NaN ì²˜ë¦¬
        features = features.fillna(method='ffill').fillna(0)

        return features

    def create_labels_with_noise_filter(self, df, timeframe):
        """ë…¸ì´ì¦ˆ í•„í„°ë§ëœ ë¼ë²¨ ìƒì„±"""
        # íƒ€ì„í”„ë ˆì„ë³„ ì ì‘ì  ì„ê³„ê°’
        thresholds = {
            '15m': 0.0015,  # 0.15%
            '30m': 0.002,   # 0.2%
            '1h': 0.003,    # 0.3%
            '4h': 0.005     # 0.5%
        }

        threshold = thresholds.get(timeframe, 0.002)

        # ë¯¸ë˜ ìˆ˜ìµë¥  (ì—¬ëŸ¬ ê¸°ê°„ ê³ ë ¤)
        future_returns = pd.DataFrame(index=df.index)
        future_returns['r1'] = df['close'].shift(-1) / df['close'] - 1
        future_returns['r2'] = df['close'].shift(-2) / df['close'] - 1
        future_returns['r3'] = df['close'].shift(-3) / df['close'] - 1

        # ê°€ì¤‘ í‰ê·  ë¯¸ë˜ ìˆ˜ìµë¥ 
        weighted_return = (future_returns['r1'] * 0.5 +
                          future_returns['r2'] * 0.3 +
                          future_returns['r3'] * 0.2)

        # ë¼ë²¨ ìƒì„± (ëª…í™•í•œ ì‹ í˜¸ë§Œ)
        labels = pd.Series(index=df.index, dtype=int)
        labels[weighted_return > threshold] = 1  # UP
        labels[weighted_return < -threshold] = 0  # DOWN

        # ë…¸ì´ì¦ˆ ì œê±° (ì„ê³„ê°’ ë‚´ ë³€ë™ì€ ì œì™¸)
        labels[(weighted_return >= -threshold) & (weighted_return <= threshold)] = np.nan

        return labels

    def train_ensemble_model(self, timeframe):
        """ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ {timeframe} ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨")
        print(f"{'='*60}")

        # ë°ì´í„° ìˆ˜ì§‘
        df = self.get_data(timeframe, limit=10000)

        # íŠ¹ì§• ìƒì„±
        features = self.create_advanced_features(df)

        # ë¼ë²¨ ìƒì„±
        labels = self.create_labels_with_noise_filter(df, timeframe)

        # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
        valid_idx = ~(features.isna().any(axis=1) | labels.isna())
        X = features[valid_idx]
        y = labels[valid_idx]

        print(f"í›ˆë ¨ ë°ì´í„°: {len(X)}ê°œ ìƒ˜í”Œ")
        print(f"UP: {(y==1).sum()}ê°œ, DOWN: {(y==0).sum()}ê°œ")

        # ì‹œê³„ì—´ ë¶„í• 
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # ê°œë³„ ëª¨ë¸ ì •ì˜
        rf_model = RandomForestClassifier(
            n_estimators=300,
            max_depth=15,
            min_samples_split=10,
            min_samples_leaf=5,
            max_features='sqrt',
            random_state=42,
            n_jobs=-1
        )

        gb_model = GradientBoostingClassifier(
            n_estimators=300,
            learning_rate=0.05,
            max_depth=7,
            min_samples_split=10,
            min_samples_leaf=5,
            subsample=0.8,
            random_state=42
        )

        xgb_model = XGBClassifier(
            n_estimators=300,
            learning_rate=0.05,
            max_depth=7,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss'
        )

        nn_model = MLPClassifier(
            hidden_layer_sizes=(100, 50, 25),
            activation='relu',
            learning_rate='adaptive',
            learning_rate_init=0.001,
            max_iter=1000,
            early_stopping=True,
            validation_fraction=0.1,
            random_state=42
        )

        # ì•™ìƒë¸” ëª¨ë¸ (Voting)
        ensemble = VotingClassifier(
            estimators=[
                ('rf', rf_model),
                ('gb', gb_model),
                ('xgb', xgb_model),
                ('nn', nn_model)
            ],
            voting='soft',  # í™•ë¥  ê¸°ë°˜ íˆ¬í‘œ
            weights=[1, 1.5, 1.5, 1]  # XGBoostì™€ GradientBoostingì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        )

        # í›ˆë ¨
        print("ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        ensemble.fit(X_train_scaled, y_train)

        # í‰ê°€
        y_pred_train = ensemble.predict(X_train_scaled)
        y_pred_test = ensemble.predict(X_test_scaled)
        y_proba_test = ensemble.predict_proba(X_test_scaled)

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        train_acc = accuracy_score(y_train, y_pred_train)
        test_acc = accuracy_score(y_test, y_pred_test)
        precision = precision_score(y_test, y_pred_test)
        recall = recall_score(y_test, y_pred_test)
        f1 = f1_score(y_test, y_pred_test)

        print(f"\nğŸ“Š ëª¨ë¸ í‰ê°€:")
        print(f"í›ˆë ¨ ì •í™•ë„: {train_acc*100:.1f}%")
        print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc*100:.1f}%")
        print(f"ì •ë°€ë„: {precision*100:.1f}%")
        print(f"ì¬í˜„ìœ¨: {recall*100:.1f}%")
        print(f"F1 ì ìˆ˜: {f1:.3f}")

        # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥
        print(f"\nğŸ“ˆ ê°œë³„ ëª¨ë¸ ì„±ëŠ¥:")
        for estimator in ensemble.estimators_:
            name = estimator[0]
            model = estimator[1]
            model.fit(X_train_scaled, y_train)
            individual_acc = accuracy_score(y_test, model.predict(X_test_scaled))
            print(f"  {name.upper()}: {individual_acc*100:.1f}%")

        # ì˜ˆì¸¡ ë¶„í¬
        pred_dist = pd.Series(y_pred_test).value_counts()
        print(f"\nì˜ˆì¸¡ ë¶„í¬:")
        print(f"  DOWN: {pred_dist.get(0, 0)}ê°œ ({pred_dist.get(0, 0)/len(y_pred_test)*100:.1f}%)")
        print(f"  UP: {pred_dist.get(1, 0)}ê°œ ({pred_dist.get(1, 0)/len(y_pred_test)*100:.1f}%)")

        # ì‹ ë¢°ë„ ë¶„ì„
        confidence_scores = np.max(y_proba_test, axis=1)
        print(f"\nì‹ ë¢°ë„ ë¶„ì„:")
        print(f"  í‰ê· : {confidence_scores.mean():.1%}")
        print(f"  ìµœì†Œ: {confidence_scores.min():.1%}")
        print(f"  ìµœëŒ€: {confidence_scores.max():.1%}")

        # ëª¨ë¸ ì €ì¥
        if test_acc > 0.55:  # 55% ì´ìƒì¸ ê²½ìš°ë§Œ ì €ì¥
            model_info = {
                'model': ensemble,
                'scaler': scaler,
                'features': list(features.columns),
                'accuracy': test_acc,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'timeframe': timeframe,
                'trained_at': datetime.now().isoformat()
            }

            filename = f"improved_{timeframe}_ensemble_model.pkl"
            joblib.dump(model_info, f"models/{filename}")
            print(f"\nâœ… ëª¨ë¸ ì €ì¥: models/{filename}")

            return model_info
        else:
            print(f"\nâš ï¸ ì •í™•ë„ê°€ ë‚®ì•„ ì €ì¥í•˜ì§€ ì•ŠìŒ ({test_acc*100:.1f}%)")
            return None

def main():
    trainer = ImprovedModelTrainer()

    print("=" * 60)
    print("ğŸ”§ ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
    print("=" * 60)

    results = {}
    for timeframe in ['15m', '30m', '1h', '4h']:
        model_info = trainer.train_ensemble_model(timeframe)
        if model_info:
            results[timeframe] = model_info

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“‹ í›ˆë ¨ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    for tf, info in results.items():
        print(f"\n{tf}:")
        print(f"  ì •í™•ë„: {info['accuracy']*100:.1f}%")
        print(f"  F1 ì ìˆ˜: {info['f1_score']:.3f}")

if __name__ == "__main__":
    main()