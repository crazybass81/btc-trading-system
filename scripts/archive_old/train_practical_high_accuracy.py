#!/usr/bin/env python3
"""
ì‹¤ìš©ì ì¸ ê³ ì •í™•ë„ ëª¨ë¸ í›ˆë ¨
- GridSearch ì—†ì´ ê²€ì¦ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
- ë¹ ë¥¸ í›ˆë ¨ ì‹œê°„
- ì‹¤ì „ ì‚¬ìš© ê°€ëŠ¥
"""

import ccxt
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
import joblib
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class PracticalHighAccuracyTrainer:
    def __init__(self):
        self.exchange = ccxt.binance()

    def get_data(self, timeframe, limit=10000):
        """ë°ì´í„° ìˆ˜ì§‘"""
        print(f"ğŸ“Š ë°ì´í„° ìˆ˜ì§‘: {timeframe}")

        all_data = []
        chunk_size = 1000

        for i in range(limit // chunk_size):
            try:
                since = None
                if all_data:
                    since = all_data[-1][0] - (chunk_size * 60000)

                ohlcv = self.exchange.fetch_ohlcv(
                    'BTC/USDT', timeframe, limit=chunk_size, since=since
                )

                if all_data:
                    ohlcv = [x for x in ohlcv if x[0] < all_data[0][0]]

                all_data = ohlcv + all_data

                if len(all_data) >= limit:
                    break

            except Exception as e:
                print(f"  âš ï¸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ë‹¨: {e}")
                break

        df = pd.DataFrame(all_data[:limit], columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)

        print(f"  âœ… {len(df)}ê°œ ìº”ë“¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return df

    def create_practical_features(self, df):
        """ì‹¤ìš©ì ì¸ í•µì‹¬ íŠ¹ì§•ë§Œ ìƒì„±"""
        features = pd.DataFrame(index=df.index)

        # 1. ê°€ê²© ë³€í™”ìœ¨ (í•µì‹¬)
        for period in [1, 3, 5, 10, 20, 50]:
            features[f'return_{period}'] = df['close'].pct_change(period)

        # 2. ì´ë™í‰ê· 
        for period in [10, 20, 50, 100]:
            ma = df['close'].rolling(window=period).mean()
            features[f'ma_{period}_ratio'] = (df['close'] - ma) / ma
            features[f'ma_{period}_slope'] = ma.pct_change(5)

        # 3. RSI
        for period in [14, 21]:
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / (loss + 1e-10)
            features[f'rsi_{period}'] = 100 - (100 / (1 + rs))

        # 4. MACD
        exp1 = df['close'].ewm(span=12, adjust=False).mean()
        exp2 = df['close'].ewm(span=26, adjust=False).mean()
        macd = exp1 - exp2
        signal = macd.ewm(span=9, adjust=False).mean()
        features['macd'] = macd / df['close']
        features['macd_signal'] = signal / df['close']
        features['macd_hist'] = (macd - signal) / df['close']

        # 5. ë³¼ë¦°ì € ë°´ë“œ
        for period in [20]:
            ma = df['close'].rolling(window=period).mean()
            std = df['close'].rolling(window=period).std()
            features[f'bb_{period}_position'] = (df['close'] - ma) / (2 * std)
            features[f'bb_{period}_width'] = (2 * std) / ma

        # 6. ATR (ë³€ë™ì„±)
        high_low = df['high'] - df['low']
        high_close = abs(df['high'] - df['close'].shift())
        low_close = abs(df['low'] - df['close'].shift())
        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        features['atr_14'] = tr.rolling(window=14).mean() / df['close']

        # 7. ë³¼ë¥¨
        features['volume_ratio'] = df['volume'] / df['volume'].rolling(window=20).mean()
        features['volume_ma_ratio'] = df['volume'].rolling(window=5).mean() / df['volume'].rolling(window=20).mean()

        # 8. ì§€ì§€/ì €í•­
        features['dist_from_high_20'] = (df['high'].rolling(window=20).max() - df['close']) / df['close']
        features['dist_from_low_20'] = (df['close'] - df['low'].rolling(window=20).min()) / df['close']

        # 9. ìº”ë“¤ íŒ¨í„´
        body = df['close'] - df['open']
        features['body_ratio'] = body / (df['high'] - df['low'] + 1e-10)
        features['upper_shadow'] = (df['high'] - df[['close', 'open']].max(axis=1)) / (df['high'] - df['low'] + 1e-10)
        features['lower_shadow'] = (df[['close', 'open']].min(axis=1) - df['low']) / (df['high'] - df['low'] + 1e-10)

        # 10. ì‹œê°„ íŠ¹ì§•
        features['hour'] = df.index.hour
        features['day_of_week'] = df.index.dayofweek

        return features.fillna(0)

    def create_smart_labels(self, df, timeframe):
        """ìŠ¤ë§ˆíŠ¸ ë¼ë²¨ ìƒì„±"""
        thresholds = {
            '15m': 0.0015,
            '30m': 0.002,
            '1h': 0.0025,
            '4h': 0.003
        }

        threshold = thresholds.get(timeframe, 0.002)

        # ë¯¸ë˜ ìˆ˜ìµë¥  (ë³µí•©)
        returns = []
        weights = [0.5, 0.3, 0.2]

        for i, w in enumerate(weights, 1):
            ret = (df['close'].shift(-i) / df['close'] - 1)
            returns.append(ret * w)

        weighted_return = sum(returns)

        # ë¼ë²¨ ìƒì„±
        labels = pd.Series(index=df.index, dtype=int)
        labels[weighted_return > threshold] = 1  # UP
        labels[weighted_return < -threshold] = 0  # DOWN

        # ì• ë§¤í•œ ê²½ìš° ì œê±°
        labels = labels.dropna()

        return labels

    def train_optimized_models(self, timeframe, direction='both'):
        """ìµœì í™”ëœ ëª¨ë¸ í›ˆë ¨"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ {timeframe} ì‹¤ìš©ì  ê³ ì •í™•ë„ ëª¨ë¸ í›ˆë ¨")
        print(f"{'='*60}")

        # ë°ì´í„° ìˆ˜ì§‘
        df = self.get_data(timeframe, limit=10000)

        # íŠ¹ì§• ìƒì„±
        print("  ğŸ“ íŠ¹ì§• ìƒì„± ì¤‘...")
        features = self.create_practical_features(df)

        # ë¼ë²¨ ìƒì„±
        labels = self.create_smart_labels(df, timeframe)

        # ìœ íš¨ ë°ì´í„°
        valid_idx = features.index.intersection(labels.index)
        X = features.loc[valid_idx]
        y = labels.loc[valid_idx]

        # NaN ì œê±°
        valid_mask = ~(X.isna().any(axis=1) | y.isna())
        X = X[valid_mask]
        y = y[valid_mask]

        print(f"  ğŸ“Š ë°ì´í„°: {len(X)}ê°œ ìƒ˜í”Œ")
        print(f"  ğŸ“ˆ UP: {(y==1).sum()}ê°œ ({(y==1).sum()/len(y)*100:.1f}%)")
        print(f"  ğŸ“‰ DOWN: {(y==0).sum()}ê°œ ({(y==0).sum()/len(y)*100:.1f}%)")

        if len(X) < 100:
            print("  âš ï¸ ë°ì´í„° ë¶€ì¡±")
            return None

        # ë°ì´í„° ë¶„í• 
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # ëª¨ë¸ ì •ì˜ (ê²€ì¦ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°)
        models = {
            'xgboost': XGBClassifier(
                n_estimators=300,
                max_depth=7,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                use_label_encoder=False,
                eval_metric='logloss'
            ),
            'lightgbm': LGBMClassifier(
                n_estimators=300,
                max_depth=7,
                learning_rate=0.05,
                num_leaves=50,
                min_child_samples=20,
                random_state=42,
                verbosity=-1
            ),
            'rf': RandomForestClassifier(
                n_estimators=300,
                max_depth=15,
                min_samples_split=10,
                min_samples_leaf=5,
                max_features='sqrt',
                random_state=42,
                n_jobs=-1
            ),
            'gb': GradientBoostingClassifier(
                n_estimators=200,
                max_depth=5,
                learning_rate=0.05,
                min_samples_split=20,
                min_samples_leaf=10,
                subsample=0.8,
                random_state=42
            )
        }

        # ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
        print("\n  ğŸ“Š ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€:")
        results = {}

        for name, model in models.items():
            print(f"    {name}...", end=' ')

            # í›ˆë ¨
            model.fit(X_train_scaled, y_train)

            # ì˜ˆì¸¡
            y_pred = model.predict(X_test_scaled)
            y_proba = model.predict_proba(X_test_scaled)

            # í‰ê°€
            acc = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, zero_division=0)
            recall = recall_score(y_test, y_pred, zero_division=0)
            f1 = f1_score(y_test, y_pred, zero_division=0)

            results[name] = {
                'model': model,
                'accuracy': acc,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'predictions': y_pred,
                'probabilities': y_proba
            }

            print(f"Acc={acc:.3f}, F1={f1:.3f}")

        # ì•™ìƒë¸” ì˜ˆì¸¡
        print("\n  ğŸ¯ ì•™ìƒë¸” ì˜ˆì¸¡:")

        # ì†Œí”„íŠ¸ ë³´íŒ…
        ensemble_proba = np.zeros((len(X_test_scaled), 2))
        weights = {'xgboost': 1.5, 'lightgbm': 1.5, 'rf': 1.0, 'gb': 1.2}

        for name, result in results.items():
            weight = weights.get(name, 1.0)
            ensemble_proba += result['probabilities'] * weight

        ensemble_proba /= sum(weights.values())
        ensemble_pred = (ensemble_proba[:, 1] > 0.5).astype(int)

        # ì•™ìƒë¸” í‰ê°€
        ensemble_acc = accuracy_score(y_test, ensemble_pred)
        ensemble_precision = precision_score(y_test, ensemble_pred, zero_division=0)
        ensemble_recall = recall_score(y_test, ensemble_pred, zero_division=0)
        ensemble_f1 = f1_score(y_test, ensemble_pred, zero_division=0)

        print(f"    ì•™ìƒë¸”: Acc={ensemble_acc:.3f}, P={ensemble_precision:.3f}, R={ensemble_recall:.3f}, F1={ensemble_f1:.3f}")

        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y_test, ensemble_pred)
        print(f"\n  í˜¼ë™ í–‰ë ¬:")
        print(f"           ì˜ˆì¸¡DOWN  ì˜ˆì¸¡UP")
        print(f"  ì‹¤ì œDOWN    {cm[0,0]:4d}    {cm[0,1]:4d}")
        print(f"  ì‹¤ì œUP      {cm[1,0]:4d}    {cm[1,1]:4d}")

        # ê° í´ë˜ìŠ¤ ì •í™•ë„
        if cm[0,0] + cm[0,1] > 0:
            down_acc = cm[0,0] / (cm[0,0] + cm[0,1]) * 100
            print(f"\n  DOWN ì˜ˆì¸¡ ì •í™•ë„: {down_acc:.1f}%")
        if cm[1,0] + cm[1,1] > 0:
            up_acc = cm[1,1] / (cm[1,0] + cm[1,1]) * 100
            print(f"  UP ì˜ˆì¸¡ ì •í™•ë„: {up_acc:.1f}%")

        # ëª¨ë¸ ì €ì¥
        if ensemble_acc > 0.55:  # 55% ì´ìƒë§Œ ì €ì¥
            model_info = {
                'models': {name: r['model'] for name, r in results.items()},
                'scaler': scaler,
                'features': list(features.columns),
                'ensemble_accuracy': ensemble_acc,
                'ensemble_precision': ensemble_precision,
                'ensemble_recall': ensemble_recall,
                'ensemble_f1': ensemble_f1,
                'individual_results': {
                    name: {
                        'accuracy': r['accuracy'],
                        'f1': r['f1']
                    } for name, r in results.items()
                },
                'timeframe': timeframe,
                'data_size': len(X),
                'trained_at': datetime.now().isoformat()
            }

            filename = f"practical_high_acc_{timeframe}_model.pkl"
            joblib.dump(model_info, f"models/{filename}")
            print(f"\n  âœ… ëª¨ë¸ ì €ì¥: models/{filename}")

            return model_info
        else:
            print(f"\n  âš ï¸ ì •í™•ë„ ë¶€ì¡± ({ensemble_acc:.1%})")
            return None

def main():
    trainer = PracticalHighAccuracyTrainer()

    print("=" * 60)
    print("ğŸ”§ ì‹¤ìš©ì  ê³ ì •í™•ë„ ëª¨ë¸ í›ˆë ¨")
    print("â° ì˜ˆìƒ ì‹œê°„: íƒ€ì„í”„ë ˆì„ë‹¹ 1-2ë¶„")
    print("=" * 60)

    results = {}

    for timeframe in ['15m', '30m', '1h', '4h']:
        try:
            model_info = trainer.train_optimized_models(timeframe)
            if model_info:
                results[timeframe] = model_info
        except Exception as e:
            print(f"\nâŒ {timeframe} í›ˆë ¨ ì‹¤íŒ¨: {e}")

    # ê²°ê³¼ ìš”ì•½
    if results:
        print("\n" + "=" * 60)
        print("ğŸ“‹ í›ˆë ¨ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)

        for tf, info in results.items():
            print(f"\n{tf}:")
            print(f"  ì•™ìƒë¸” ì •í™•ë„: {info['ensemble_accuracy']*100:.1f}%")
            print(f"  F1 ì ìˆ˜: {info['ensemble_f1']:.3f}")

            # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥
            print(f"  ê°œë³„ ëª¨ë¸:")
            for name, res in info['individual_results'].items():
                print(f"    {name}: Acc={res['accuracy']:.3f}, F1={res['f1']:.3f}")

if __name__ == "__main__":
    main()