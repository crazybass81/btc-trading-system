#!/usr/bin/env python3
"""
ì‹¤ìš©ì  ML ëª¨ë¸ í•™ìŠµ
í˜„ì‹¤ì ì¸ ëª©í‘œ: 55-60% ì •í™•ë„
"""

import pandas as pd
import numpy as np
from datetime import datetime
import ccxt
import joblib
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from loguru import logger
import warnings
warnings.filterwarnings('ignore')

class PracticalMLTrainer:
    def __init__(self):
        self.exchange = ccxt.binance()
        self.models = {}
        self.scalers = {}

    def prepare_features(self, df):
        """ê°„ë‹¨í•˜ì§€ë§Œ íš¨ê³¼ì ì¸ íŠ¹ì§• ìƒì„±"""
        features = pd.DataFrame()

        # ê°€ê²© ë³€í™”ìœ¨
        for i in [1, 3, 5, 10]:
            features[f'return_{i}'] = df['close'].pct_change(i)

        # RSI
        for period in [7, 14, 21]:
            features[f'rsi_{period}'] = self.calculate_rsi(df['close'], period)

        # MACD
        exp1 = df['close'].ewm(span=12).mean()
        exp2 = df['close'].ewm(span=26).mean()
        features['macd'] = exp1 - exp2
        features['macd_signal'] = features['macd'].ewm(span=9).mean()
        features['macd_hist'] = features['macd'] - features['macd_signal']

        # ë³¼ë¦°ì € ë°´ë“œ
        for period in [10, 20]:
            sma = df['close'].rolling(period).mean()
            std = df['close'].rolling(period).std()
            features[f'bb_position_{period}'] = (df['close'] - sma) / (2 * std)

        # ë³¼ë¥¨
        features['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
        features['volume_change'] = df['volume'].pct_change()

        # ê³ ì € ë²”ìœ„
        features['high_low_ratio'] = (df['high'] - df['low']) / df['close']
        features['close_position'] = (df['close'] - df['low']) / (df['high'] - df['low'])

        return features

    def calculate_rsi(self, prices, period=14):
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))

    def create_labels(self, df, threshold=0.002):
        """3í´ë˜ìŠ¤ ë ˆì´ë¸”: ìƒìŠ¹/í•˜ë½/ì¤‘ë¦½"""
        future_return = df['close'].shift(-1) / df['close'] - 1
        labels = pd.Series(1, index=df.index)  # ê¸°ë³¸ê°’ ì¤‘ë¦½

        labels[future_return > threshold] = 2  # ìƒìŠ¹
        labels[future_return < -threshold] = 0  # í•˜ë½

        return labels

    def train_model(self, timeframe='15m'):
        """ë‹¨ì¼ íƒ€ì„í”„ë ˆì„ ëª¨ë¸ í•™ìŠµ"""
        logger.info(f"\n{'='*50}")
        logger.info(f"ğŸš€ {timeframe} ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        logger.info(f"{'='*50}")

        # ë°ì´í„° ìˆ˜ì§‘
        logger.info("ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        ohlcv = self.exchange.fetch_ohlcv('BTC/USDT', timeframe, limit=1000)
        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])

        # íŠ¹ì§• ìƒì„±
        logger.info("íŠ¹ì§• ìƒì„± ì¤‘...")
        features = self.prepare_features(df)

        # ë ˆì´ë¸” ìƒì„±
        labels = self.create_labels(df)

        # ë°ì´í„° ì •ë¦¬
        X = features.dropna()
        y = labels[X.index]
        X = X[:-1]  # ë§ˆì§€ë§‰ í–‰ ì œê±° (ë¯¸ë˜ ë ˆì´ë¸” ì—†ìŒ)
        y = y[:-1]

        logger.info(f"ë°ì´í„° í¬ê¸°: {X.shape}")
        logger.info(f"í´ë˜ìŠ¤ ë¶„í¬: {y.value_counts().to_dict()}")

        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, shuffle=False
        )

        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # ëª¨ë¸ í•™ìŠµ
        logger.info("ëª¨ë¸ í•™ìŠµ ì¤‘...")

        # 1. Random Forest
        rf = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=10,
            random_state=42
        )
        rf.fit(X_train_scaled, y_train)
        rf_pred = rf.predict(X_test_scaled)
        rf_acc = accuracy_score(y_test, rf_pred)

        # 2. Gradient Boosting
        gb = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            random_state=42
        )
        gb.fit(X_train_scaled, y_train)
        gb_pred = gb.predict(X_test_scaled)
        gb_acc = accuracy_score(y_test, gb_pred)

        # ì•™ìƒë¸” (íˆ¬í‘œ)
        ensemble_pred = np.round((rf_pred + gb_pred) / 2).astype(int)
        ensemble_acc = accuracy_score(y_test, ensemble_pred)

        logger.info(f"\nğŸ“Š í•™ìŠµ ê²°ê³¼:")
        logger.info(f"Random Forest ì •í™•ë„: {rf_acc:.1%}")
        logger.info(f"Gradient Boosting ì •í™•ë„: {gb_acc:.1%}")
        logger.info(f"ì•™ìƒë¸” ì •í™•ë„: {ensemble_acc:.1%}")

        # ìµœê³  ëª¨ë¸ ì„ íƒ
        if ensemble_acc >= max(rf_acc, gb_acc):
            self.models[timeframe] = {'rf': rf, 'gb': gb, 'type': 'ensemble'}
            best_acc = ensemble_acc
            logger.success(f"âœ… ì•™ìƒë¸” ëª¨ë¸ ì„ íƒ (ì •í™•ë„: {ensemble_acc:.1%})")
        elif rf_acc > gb_acc:
            self.models[timeframe] = {'model': rf, 'type': 'rf'}
            best_acc = rf_acc
            logger.success(f"âœ… Random Forest ì„ íƒ (ì •í™•ë„: {rf_acc:.1%})")
        else:
            self.models[timeframe] = {'model': gb, 'type': 'gb'}
            best_acc = gb_acc
            logger.success(f"âœ… Gradient Boosting ì„ íƒ (ì •í™•ë„: {gb_acc:.1%})")

        self.scalers[timeframe] = scaler

        # ìƒì„¸ ë¦¬í¬íŠ¸
        if best_acc > 0.55:
            logger.info(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„±! (55% ì´ìƒ)")
        else:
            logger.warning(f"\nâš ï¸ ëª©í‘œ ë¯¸ë‹¬ (í˜„ì¬: {best_acc:.1%}, ëª©í‘œ: 55%)")

        return best_acc

    def save_models(self):
        """ëª¨ë¸ ì €ì¥"""
        for timeframe, model_dict in self.models.items():
            model_path = f'models/practical_{timeframe}_model.pkl'
            scaler_path = f'models/practical_{timeframe}_scaler.pkl'

            joblib.dump(model_dict, model_path)
            joblib.dump(self.scalers[timeframe], scaler_path)

            logger.info(f"âœ… {timeframe} ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")

    def train_all_timeframes(self):
        """ëª¨ë“  íƒ€ì„í”„ë ˆì„ í•™ìŠµ"""
        results = {}

        for timeframe in ['5m', '15m', '1h', '4h']:
            try:
                accuracy = self.train_model(timeframe)
                results[timeframe] = accuracy
            except Exception as e:
                logger.error(f"{timeframe} í•™ìŠµ ì‹¤íŒ¨: {e}")
                results[timeframe] = 0

        return results

def main():
    trainer = PracticalMLTrainer()

    logger.info("="*70)
    logger.info("ğŸ¤– ì‹¤ìš©ì  ML ëª¨ë¸ í•™ìŠµ")
    logger.info("ëª©í‘œ: 55-60% ì •í™•ë„ (í˜„ì‹¤ì  ëª©í‘œ)")
    logger.info("="*70)

    # ëª¨ë“  íƒ€ì„í”„ë ˆì„ í•™ìŠµ
    results = trainer.train_all_timeframes()

    # ê²°ê³¼ ìš”ì•½
    logger.info("\n" + "="*70)
    logger.info("ğŸ“‹ í•™ìŠµ ê²°ê³¼ ìš”ì•½")
    logger.info("="*70)

    total_models = len(results)
    successful_models = sum(1 for acc in results.values() if acc > 0.55)

    for timeframe, accuracy in results.items():
        status = "âœ…" if accuracy > 0.55 else "âŒ"
        logger.info(f"{timeframe}: {accuracy:.1%} {status}")

    logger.info(f"\nì„±ê³µë¥ : {successful_models}/{total_models}")

    # ëª¨ë¸ ì €ì¥
    if successful_models > 0:
        trainer.save_models()
        logger.success(f"\nâœ… {successful_models}ê°œ ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

    # í˜„ì‹¤ì  ì¡°ì–¸
    logger.info("\n" + "="*70)
    logger.info("ğŸ’¡ ì‚¬ìš©ìë‹˜ê»˜")
    logger.info("="*70)
    logger.info("\nì´ ML ëª¨ë¸ì˜ í˜„ì‹¤:")
    logger.info("1. ì •í™•ë„ëŠ” 55-60% ìˆ˜ì¤€ì´ í•œê³„")
    logger.info("2. BTCëŠ” ë³¸ì§ˆì ìœ¼ë¡œ ì˜ˆì¸¡ ì–´ë ¤ì›€")
    logger.info("3. ê³¼ë„í•œ ì‹ ë¢°ëŠ” ìœ„í—˜")
    logger.info("4. ë¦¬ìŠ¤í¬ ê´€ë¦¬ì™€ í•¨ê»˜ ì‚¬ìš© í•„ìˆ˜")
    logger.info("5. ë³´ì¡° ì§€í‘œë¡œë§Œ í™œìš© ê¶Œì¥")

if __name__ == "__main__":
    main()