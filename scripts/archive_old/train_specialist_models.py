#!/usr/bin/env python3
"""
ì „ë¬¸í™”ëœ ìƒìŠ¹/í•˜ë½ ëª¨ë¸ í›ˆë ¨
- ìƒìŠ¹ ì „ë¬¸ ëª¨ë¸: ìƒìŠ¹ íŒ¨í„´ë§Œ í•™ìŠµ
- í•˜ë½ ì „ë¬¸ ëª¨ë¸: í•˜ë½ íŒ¨í„´ë§Œ í•™ìŠµ
- ê²°í•© ì˜ˆì¸¡: ë‘ ëª¨ë¸ì˜ ì‹ ë¢°ë„ ë¹„êµ
"""

import ccxt
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from xgboost import XGBClassifier
import joblib
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class SpecialistModelTrainer:
    def __init__(self):
        self.exchange = ccxt.binance()

    def get_data(self, timeframe, limit=10000):
        """ë°ì´í„° ìˆ˜ì§‘"""
        print(f"ğŸ“Š ë°ì´í„° ìˆ˜ì§‘: {timeframe} ({limit}ê°œ ìº”ë“¤)")

        ohlcv = self.exchange.fetch_ohlcv('BTC/USDT', timeframe=timeframe, limit=limit)
        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)

        return df

    def create_specialized_features(self, df, direction='up'):
        """ë°©í–¥ë³„ íŠ¹í™” íŠ¹ì§• ìƒì„±"""
        features = pd.DataFrame(index=df.index)

        # ê³µí†µ íŠ¹ì§•
        # ê°€ê²© ë³€í™”ìœ¨
        for period in [1, 3, 5, 10, 20]:
            features[f'return_{period}'] = df['close'].pct_change(period)

        # RSI
        for period in [7, 14, 21]:
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / (loss + 1e-10)
            features[f'rsi_{period}'] = 100 - (100 / (1 + rs))

        # ì´ë™í‰ê· 
        for period in [10, 20, 50, 100]:
            ma = df['close'].rolling(window=period).mean()
            features[f'ma_{period}_ratio'] = (df['close'] - ma) / ma

        # MACD
        exp1 = df['close'].ewm(span=12, adjust=False).mean()
        exp2 = df['close'].ewm(span=26, adjust=False).mean()
        macd = exp1 - exp2
        signal = macd.ewm(span=9, adjust=False).mean()
        features['macd'] = macd / df['close']
        features['macd_signal'] = signal / df['close']
        features['macd_hist'] = (macd - signal) / df['close']

        # ë³¼ë¦°ì € ë°´ë“œ
        for period in [20, 50]:
            ma = df['close'].rolling(window=period).mean()
            std = df['close'].rolling(window=period).std()
            features[f'bb_{period}_upper'] = (df['close'] - (ma + 2*std)) / df['close']
            features[f'bb_{period}_lower'] = ((ma - 2*std) - df['close']) / df['close']
            features[f'bb_{period}_width'] = (4 * std) / ma

        # ê±°ë˜ëŸ‰
        features['volume_ratio'] = df['volume'] / df['volume'].rolling(window=20).mean()
        features['volume_change'] = df['volume'].pct_change()

        if direction == 'up':
            # ìƒìŠ¹ íŠ¹í™” íŠ¹ì§•
            # ì§€ì§€ì„  ëŒíŒŒ
            features['support_break'] = df['close'] / df['low'].rolling(window=20).min() - 1

            # ìƒìŠ¹ ëª¨ë©˜í…€
            features['up_momentum'] = (df['close'] > df['close'].shift(1)).rolling(window=10).sum()

            # ê³ ì  ê°±ì‹ 
            features['new_high_20'] = (df['high'] == df['high'].rolling(window=20).max()).astype(int)
            features['new_high_50'] = (df['high'] == df['high'].rolling(window=50).max()).astype(int)

            # ì–‘ë´‰ ë¹„ìœ¨
            features['bullish_ratio'] = ((df['close'] > df['open']).rolling(window=10).sum()) / 10

            # ìƒìŠ¹ ê±°ë˜ëŸ‰
            up_days = df['close'] > df['close'].shift(1)
            features['up_volume'] = (df['volume'] * up_days).rolling(window=10).sum()

            # ê³¨ë“ í¬ë¡œìŠ¤
            ma50 = df['close'].rolling(window=50).mean()
            ma200 = df['close'].rolling(window=200).mean()
            features['golden_cross'] = ((ma50 > ma200) & (ma50.shift(1) <= ma200.shift(1))).astype(int)

        else:  # down
            # í•˜ë½ íŠ¹í™” íŠ¹ì§•
            # ì €í•­ì„  ëŒíŒŒ
            features['resistance_break'] = 1 - df['close'] / df['high'].rolling(window=20).max()

            # í•˜ë½ ëª¨ë©˜í…€
            features['down_momentum'] = (df['close'] < df['close'].shift(1)).rolling(window=10).sum()

            # ì €ì  ê°±ì‹ 
            features['new_low_20'] = (df['low'] == df['low'].rolling(window=20).min()).astype(int)
            features['new_low_50'] = (df['low'] == df['low'].rolling(window=50).min()).astype(int)

            # ìŒë´‰ ë¹„ìœ¨
            features['bearish_ratio'] = ((df['close'] < df['open']).rolling(window=10).sum()) / 10

            # í•˜ë½ ê±°ë˜ëŸ‰
            down_days = df['close'] < df['close'].shift(1)
            features['down_volume'] = (df['volume'] * down_days).rolling(window=10).sum()

            # ë°ë“œí¬ë¡œìŠ¤
            ma50 = df['close'].rolling(window=50).mean()
            ma200 = df['close'].rolling(window=200).mean()
            features['death_cross'] = ((ma50 < ma200) & (ma50.shift(1) >= ma200.shift(1))).astype(int)

        # ë³€ë™ì„±
        features['volatility'] = df['close'].pct_change().rolling(window=20).std()
        features['high_low_ratio'] = (df['high'] - df['low']) / df['close']

        # ì‹œê°„ íŠ¹ì§•
        features['hour'] = df.index.hour
        features['day_of_week'] = df.index.dayofweek

        return features.fillna(0)

    def create_specialist_labels(self, df, direction='up', timeframe='15m'):
        """ì „ë¬¸í™”ëœ ë¼ë²¨ ìƒì„±"""
        # íƒ€ì„í”„ë ˆì„ë³„ ì„ê³„ê°’
        thresholds = {
            '15m': 0.001,   # 0.1%
            '30m': 0.0015,  # 0.15%
            '1h': 0.002,    # 0.2%
            '4h': 0.003     # 0.3%
        }

        threshold = thresholds.get(timeframe, 0.002)

        # ë¯¸ë˜ ìˆ˜ìµë¥ 
        future_return = df['close'].shift(-1) / df['close'] - 1

        if direction == 'up':
            # ìƒìŠ¹ ëª¨ë¸: ìƒìŠ¹ì€ 1, ë‚˜ë¨¸ì§€ëŠ” 0
            labels = (future_return > threshold).astype(int)
        else:
            # í•˜ë½ ëª¨ë¸: í•˜ë½ì€ 1, ë‚˜ë¨¸ì§€ëŠ” 0
            labels = (future_return < -threshold).astype(int)

        return labels

    def train_specialist_model(self, timeframe, direction='up'):
        """ì „ë¬¸ ëª¨ë¸ í›ˆë ¨"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ {timeframe} {direction.upper()} ì „ë¬¸ ëª¨ë¸ í›ˆë ¨")
        print(f"{'='*60}")

        # ë°ì´í„° ìˆ˜ì§‘
        df = self.get_data(timeframe)

        # íŠ¹ì§• ìƒì„±
        features = self.create_specialized_features(df, direction)

        # ë¼ë²¨ ìƒì„±
        labels = self.create_specialist_labels(df, direction, timeframe)

        # ìœ íš¨ ë°ì´í„°
        valid_idx = ~(features.isna().any(axis=1) | labels.isna())
        X = features[valid_idx]
        y = labels[valid_idx]

        print(f"í›ˆë ¨ ë°ì´í„°: {len(X)}ê°œ")
        print(f"íƒ€ê²Ÿ í´ë˜ìŠ¤ ë¹„ìœ¨: {y.mean():.1%}")

        # ë°ì´í„° ë¶„í• 
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # ëª¨ë¸ ì„ íƒ (ë°©í–¥ë³„ ìµœì í™”)
        if direction == 'up':
            # ìƒìŠ¹ ì „ë¬¸: GradientBoosting (ë³´ìˆ˜ì )
            model = GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=5,
                min_samples_split=20,
                min_samples_leaf=10,
                subsample=0.8,
                random_state=42
            )
        else:
            # í•˜ë½ ì „ë¬¸: XGBoost (ë¯¼ê°)
            model = XGBClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=5,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                use_label_encoder=False,
                eval_metric='logloss'
            )

        # í›ˆë ¨
        model.fit(X_train_scaled, y_train)

        # í‰ê°€
        y_pred = model.predict(X_test_scaled)
        y_proba = model.predict_proba(X_test_scaled)[:, 1]

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)

        try:
            auc = roc_auc_score(y_test, y_proba)
        except:
            auc = 0.5

        print(f"\nğŸ“Š ëª¨ë¸ í‰ê°€:")
        print(f"ì •í™•ë„: {accuracy*100:.1f}%")
        print(f"ì •ë°€ë„: {precision*100:.1f}%")
        print(f"ì¬í˜„ìœ¨: {recall*100:.1f}%")
        print(f"F1 ì ìˆ˜: {f1:.3f}")
        print(f"AUC: {auc:.3f}")

        # íŠ¹ì§• ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ)
        if hasattr(model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                'feature': X.columns,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)

            print(f"\nğŸ”‘ ì¤‘ìš” íŠ¹ì§• (ìƒìœ„ 10ê°œ):")
            for i, row in feature_importance.head(10).iterrows():
                print(f"  {row['feature']}: {row['importance']:.3f}")

        # ëª¨ë¸ ì •ë³´ ì €ì¥
        model_info = {
            'model': model,
            'scaler': scaler,
            'features': list(features.columns),
            'direction': direction,
            'timeframe': timeframe,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'auc': auc,
            'trained_at': datetime.now().isoformat()
        }

        return model_info

    def train_combined_specialist(self, timeframe):
        """ìƒìŠ¹/í•˜ë½ ì „ë¬¸ ëª¨ë¸ ê²°í•©"""
        print(f"\n{'='*60}")
        print(f"ğŸ¯ {timeframe} ê²°í•© ì „ë¬¸ ëª¨ë¸")
        print(f"{'='*60}")

        # ìƒìŠ¹ ì „ë¬¸ ëª¨ë¸ í›ˆë ¨
        up_model = self.train_specialist_model(timeframe, 'up')

        # í•˜ë½ ì „ë¬¸ ëª¨ë¸ í›ˆë ¨
        down_model = self.train_specialist_model(timeframe, 'down')

        # ê²°í•© ëª¨ë¸ ì €ì¥
        combined_info = {
            'up_model': up_model,
            'down_model': down_model,
            'timeframe': timeframe,
            'trained_at': datetime.now().isoformat()
        }

        filename = f"specialist_{timeframe}_combined_model.pkl"
        joblib.dump(combined_info, f"models/{filename}")
        print(f"\nâœ… ê²°í•© ëª¨ë¸ ì €ì¥: models/{filename}")

        return combined_info

    def predict_with_specialists(self, combined_model, df):
        """ì „ë¬¸ ëª¨ë¸ë¡œ ì˜ˆì¸¡"""
        up_model_info = combined_model['up_model']
        down_model_info = combined_model['down_model']

        # ìƒìŠ¹ íŠ¹ì§• ìƒì„± ë° ì˜ˆì¸¡
        up_features = self.create_specialized_features(df, 'up')
        up_features = up_features[up_model_info['features']].iloc[-1:].fillna(0)
        up_X_scaled = up_model_info['scaler'].transform(up_features)
        up_proba = up_model_info['model'].predict_proba(up_X_scaled)[0, 1]

        # í•˜ë½ íŠ¹ì§• ìƒì„± ë° ì˜ˆì¸¡
        down_features = self.create_specialized_features(df, 'down')
        down_features = down_features[down_model_info['features']].iloc[-1:].fillna(0)
        down_X_scaled = down_model_info['scaler'].transform(down_features)
        down_proba = down_model_info['model'].predict_proba(down_X_scaled)[0, 1]

        print(f"\nğŸ“Š ì „ë¬¸ ëª¨ë¸ ì˜ˆì¸¡:")
        print(f"ìƒìŠ¹ í™•ë¥ : {up_proba:.1%}")
        print(f"í•˜ë½ í™•ë¥ : {down_proba:.1%}")

        # ìµœì¢… ì˜ˆì¸¡
        if up_proba > down_proba and up_proba > 0.5:
            prediction = "UP"
            confidence = up_proba
        elif down_proba > up_proba and down_proba > 0.5:
            prediction = "DOWN"
            confidence = down_proba
        else:
            prediction = "NEUTRAL"
            confidence = max(1 - up_proba, 1 - down_proba)

        return prediction, confidence

def test_specialist_models():
    """ì „ë¬¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    trainer = SpecialistModelTrainer()
    exchange = ccxt.binance()

    print("=" * 60)
    print("ğŸ§ª ì „ë¬¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # 15ë¶„ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    timeframe = '15m'

    # ëª¨ë¸ í›ˆë ¨
    combined_model = trainer.train_combined_specialist(timeframe)

    # ìµœì‹  ë°ì´í„°ë¡œ ì˜ˆì¸¡
    ohlcv = exchange.fetch_ohlcv('BTC/USDT', timeframe, limit=200)
    df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
    df.set_index('timestamp', inplace=True)

    prediction, confidence = trainer.predict_with_specialists(combined_model, df)

    print(f"\nğŸ¯ ìµœì¢… ì˜ˆì¸¡: {prediction} (ì‹ ë¢°ë„: {confidence:.1%})")

    # ì‹¤ì œ ê°€ê²© ì •ë³´
    current_price = df['close'].iloc[-1]
    price_change = df['close'].pct_change().iloc[-1] * 100

    print(f"\ní˜„ì¬ ê°€ê²©: ${current_price:,.0f} ({price_change:+.2f}%)")

def main():
    trainer = SpecialistModelTrainer()

    print("=" * 60)
    print("ğŸ”§ ì „ë¬¸í™” ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
    print("=" * 60)

    results = {}
    for timeframe in ['15m', '30m', '1h', '4h']:
        combined_model = trainer.train_combined_specialist(timeframe)
        results[timeframe] = combined_model

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“‹ í›ˆë ¨ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    for tf, model_info in results.items():
        up_acc = model_info['up_model']['accuracy']
        down_acc = model_info['down_model']['accuracy']
        up_f1 = model_info['up_model']['f1_score']
        down_f1 = model_info['down_model']['f1_score']

        print(f"\n{tf}:")
        print(f"  ìƒìŠ¹ ëª¨ë¸: ì •í™•ë„ {up_acc*100:.1f}%, F1 {up_f1:.3f}")
        print(f"  í•˜ë½ ëª¨ë¸: ì •í™•ë„ {down_acc*100:.1f}%, F1 {down_f1:.3f}")

if __name__ == "__main__":
    main()