#!/usr/bin/env python3
"""
ê°œì„ ëœ ê³ ì •í™•ë„ ëª¨ë¸ í›ˆë ¨
- ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘
- ë°ì´í„° í’ˆì§ˆ ê°œì„ 
- íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ìµœì í™”
"""

import ccxt
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
import joblib
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class ImprovedAccuracyTrainer:
    def __init__(self):
        self.exchange = ccxt.binance()

    def get_extended_data(self, timeframe, days=90):
        """ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ (90ì¼)"""
        print(f"ğŸ“Š í™•ì¥ ë°ì´í„° ìˆ˜ì§‘: {timeframe} ({days}ì¼)")

        all_data = []
        chunk_size = 1000

        # í˜„ì¬ ì‹œê°„ë¶€í„° ê³¼ê±°ë¡œ
        end_time = self.exchange.milliseconds()

        # íƒ€ì„í”„ë ˆì„ë³„ ë°€ë¦¬ì´ˆ
        tf_ms = {
            '15m': 15 * 60 * 1000,
            '30m': 30 * 60 * 1000,
            '1h': 60 * 60 * 1000,
            '4h': 240 * 60 * 1000
        }

        ms_per_candle = tf_ms.get(timeframe, 60 * 60 * 1000)
        total_candles_needed = int(days * 24 * 60 * 60 * 1000 / ms_per_candle)

        collected = 0
        current_time = end_time

        while collected < total_candles_needed:
            try:
                # ê³¼ê±° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                ohlcv = self.exchange.fetch_ohlcv(
                    'BTC/USDT',
                    timeframe,
                    limit=chunk_size,
                    since=current_time - (chunk_size * ms_per_candle)
                )

                if not ohlcv:
                    break

                all_data = ohlcv + all_data
                collected += len(ohlcv)

                # ë‹¤ìŒ ì²­í¬ë¥¼ ìœ„í•´ ì‹œê°„ ì´ë™
                if ohlcv:
                    current_time = ohlcv[0][0]

                print(f"  ìˆ˜ì§‘ ì§„í–‰: {collected}/{total_candles_needed} ({collected/total_candles_needed*100:.1f}%)")

                if len(all_data) >= total_candles_needed:
                    all_data = all_data[-total_candles_needed:]
                    break

            except Exception as e:
                print(f"  âš ï¸ ìˆ˜ì§‘ ì¤‘ë‹¨: {e}")
                break

        df = pd.DataFrame(all_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)

        print(f"  âœ… {len(df)}ê°œ ìº”ë“¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return df

    def create_robust_features(self, df):
        """ê°œì„ ëœ íŠ¹ì§• ìƒì„± (infinity ë°©ì§€)"""
        features = pd.DataFrame(index=df.index)

        # ì•ˆì „í•œ epsilon ê°’
        eps = 1e-10

        # 1. ê°€ê²© ë³€í™”ìœ¨ (ì•ˆì „í•˜ê²Œ)
        for period in [1, 2, 3, 5, 8, 13, 21]:
            returns = df['close'].pct_change(period)
            features[f'return_{period}'] = returns.clip(-1, 1)  # ê·¹ë‹¨ê°’ ì œí•œ

        # 2. ì´ë™í‰ê·  (ì•ˆì „í•œ ê³„ì‚°)
        for period in [7, 14, 21, 50]:
            ma = df['close'].rolling(window=period, min_periods=1).mean()
            features[f'ma_{period}_ratio'] = ((df['close'] - ma) / (ma + eps)).clip(-1, 1)

        # 3. RSI (ê°œì„ ëœ ë²„ì „)
        for period in [14, 21]:
            delta = df['close'].diff()
            gain = delta.where(delta > 0, 0).rolling(window=period, min_periods=1).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()
            rs = gain / (loss + eps)
            features[f'rsi_{period}'] = (100 - (100 / (1 + rs))).clip(0, 100)

        # 4. ë³¼ë¦°ì € ë°´ë“œ (ì•ˆì „í•œ ê³„ì‚°)
        for period in [20]:
            ma = df['close'].rolling(window=period, min_periods=1).mean()
            std = df['close'].rolling(window=period, min_periods=1).std()
            features[f'bb_{period}_position'] = ((df['close'] - ma) / (2 * std + eps)).clip(-3, 3)

        # 5. ë³¼ë¥¨ ì§€í‘œ (ì•ˆì „í•œ ê³„ì‚°)
        vol_ma = df['volume'].rolling(window=20, min_periods=1).mean()
        features['volume_ratio'] = (df['volume'] / (vol_ma + eps)).clip(0, 5)

        # 6. ë³€ë™ì„± (ATR)
        high_low = df['high'] - df['low']
        high_close = abs(df['high'] - df['close'].shift())
        low_close = abs(df['low'] - df['close'].shift())
        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        features['atr_14'] = (tr.rolling(window=14, min_periods=1).mean() / (df['close'] + eps)).clip(0, 0.1)

        # 7. ì¶”ì„¸ ê°•ë„
        for period in [10, 20]:
            highest = df['high'].rolling(window=period, min_periods=1).max()
            lowest = df['low'].rolling(window=period, min_periods=1).min()
            features[f'trend_strength_{period}'] = ((df['close'] - lowest) / (highest - lowest + eps)).clip(0, 1)

        # 8. MACD (ì•ˆì „í•œ ê³„ì‚°)
        exp1 = df['close'].ewm(span=12, adjust=False).mean()
        exp2 = df['close'].ewm(span=26, adjust=False).mean()
        macd = exp1 - exp2
        signal = macd.ewm(span=9, adjust=False).mean()
        features['macd'] = (macd / (df['close'] + eps)).clip(-0.1, 0.1)
        features['macd_signal'] = (signal / (df['close'] + eps)).clip(-0.1, 0.1)

        # NaNê³¼ infinity ì œê±°
        features = features.replace([np.inf, -np.inf], np.nan)
        features = features.fillna(0)

        # ê·¹ë‹¨ê°’ ì¶”ê°€ ì œí•œ
        for col in features.columns:
            q1 = features[col].quantile(0.01)
            q99 = features[col].quantile(0.99)
            features[col] = features[col].clip(q1, q99)

        return features

    def create_balanced_labels(self, df, timeframe):
        """ê· í˜•ì¡íŒ ë¼ë²¨ ìƒì„±"""
        thresholds = {
            '15m': 0.0010,  # 0.1%
            '30m': 0.0015,  # 0.15%
            '1h': 0.0020,   # 0.2%
            '4h': 0.0030    # 0.3%
        }

        threshold = thresholds.get(timeframe, 0.002)

        # ë¯¸ë˜ ìˆ˜ìµë¥  (ë‹¤ìŒ 3ê°œ ìº”ë“¤ì˜ ê°€ì¤‘í‰ê· )
        future_returns = []
        weights = [0.5, 0.3, 0.2]

        for i, w in enumerate(weights, 1):
            ret = df['close'].shift(-i).pct_change()
            future_returns.append(ret * w)

        weighted_return = sum(future_returns)

        # ë¼ë²¨ ìƒì„±
        labels = pd.Series(index=df.index, dtype=int)
        labels[weighted_return > threshold] = 1  # UP
        labels[weighted_return < -threshold] = 0  # DOWN

        # ì¤‘ë¦½ êµ¬ê°„ ì œê±° (ëª…í™•í•œ ì‹ í˜¸ë§Œ)
        labels[(weighted_return >= -threshold) & (weighted_return <= threshold)] = np.nan
        labels = labels.dropna()

        return labels

    def train_optimized_model(self, timeframe):
        """ìµœì í™”ëœ ëª¨ë¸ í›ˆë ¨"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ {timeframe} ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨")
        print(f"{'='*60}")

        # í™•ì¥ ë°ì´í„° ìˆ˜ì§‘ (90ì¼)
        df = self.get_extended_data(timeframe, days=90)

        if len(df) < 100:
            print("  âš ï¸ ë°ì´í„° ë¶€ì¡±")
            return None

        # ì•ˆì „í•œ íŠ¹ì§• ìƒì„±
        print("  ğŸ“ ì•ˆì „í•œ íŠ¹ì§• ìƒì„± ì¤‘...")
        features = self.create_robust_features(df)

        # ê· í˜•ì¡íŒ ë¼ë²¨
        labels = self.create_balanced_labels(df, timeframe)

        # ìœ íš¨ ë°ì´í„°
        valid_idx = features.index.intersection(labels.index)
        X = features.loc[valid_idx]
        y = labels.loc[valid_idx]

        # ìµœì¢… ì •ë¦¬
        valid_mask = ~(X.isna().any(axis=1) | y.isna())
        X = X[valid_mask]
        y = y[valid_mask]

        print(f"  ğŸ“Š ë°ì´í„°: {len(X)}ê°œ ìƒ˜í”Œ")
        print(f"  ğŸ“ˆ UP: {(y==1).sum()}ê°œ ({(y==1).sum()/len(y)*100:.1f}%)")
        print(f"  ğŸ“‰ DOWN: {(y==0).sum()}ê°œ ({(y==0).sum()/len(y)*100:.1f}%)")

        if len(X) < 100:
            print("  âš ï¸ ìœ íš¨ ë°ì´í„° ë¶€ì¡±")
            return None

        # ë°ì´í„° ë¶„í• 
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        # RobustScaler ì‚¬ìš© (ì´ìƒì¹˜ì— ê°•í•¨)
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # GridSearchì—ì„œ ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©
        models = {
            'xgboost': XGBClassifier(
                n_estimators=500,
                max_depth=7,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                use_label_encoder=False,
                eval_metric='logloss'
            ),
            'lightgbm': LGBMClassifier(
                n_estimators=500,
                max_depth=7,
                learning_rate=0.05,
                num_leaves=50,
                min_child_samples=10,
                random_state=42,
                verbosity=-1,
                force_col_wise=True
            ),
            'rf': RandomForestClassifier(
                n_estimators=500,
                max_depth=15,
                min_samples_split=10,
                min_samples_leaf=5,
                max_features='sqrt',
                random_state=42,
                n_jobs=-1
            ),
            'gb': GradientBoostingClassifier(
                n_estimators=300,
                max_depth=7,
                learning_rate=0.05,
                min_samples_split=20,
                min_samples_leaf=10,
                subsample=0.8,
                random_state=42
            )
        }

        # ëª¨ë¸ í›ˆë ¨
        print("\n  ğŸ“Š ëª¨ë¸ í›ˆë ¨:")
        results = {}

        for name, model in models.items():
            print(f"    {name}...", end=' ')

            # í›ˆë ¨
            model.fit(X_train_scaled, y_train)

            # ì˜ˆì¸¡
            y_pred = model.predict(X_test_scaled)
            y_proba = model.predict_proba(X_test_scaled)

            # í‰ê°€
            acc = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, zero_division=0)
            recall = recall_score(y_test, y_pred, zero_division=0)
            f1 = f1_score(y_test, y_pred, zero_division=0)

            results[name] = {
                'model': model,
                'accuracy': acc,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'predictions': y_pred,
                'probabilities': y_proba
            }

            print(f"Acc={acc:.3f}, F1={f1:.3f}")

        # ì•™ìƒë¸” (ê°€ì¤‘ í‰ê· )
        print("\n  ğŸ¯ ì•™ìƒë¸” ì˜ˆì¸¡:")

        ensemble_proba = np.zeros((len(X_test_scaled), 2))
        weights = {'xgboost': 1.5, 'lightgbm': 1.5, 'rf': 1.2, 'gb': 1.0}

        for name, result in results.items():
            weight = weights.get(name, 1.0)
            ensemble_proba += result['probabilities'] * weight

        ensemble_proba /= sum(weights.values())
        ensemble_pred = (ensemble_proba[:, 1] > 0.5).astype(int)

        # ì•™ìƒë¸” í‰ê°€
        ensemble_acc = accuracy_score(y_test, ensemble_pred)
        ensemble_precision = precision_score(y_test, ensemble_pred, zero_division=0)
        ensemble_recall = recall_score(y_test, ensemble_pred, zero_division=0)
        ensemble_f1 = f1_score(y_test, ensemble_pred, zero_division=0)

        print(f"    ì•™ìƒë¸”: Acc={ensemble_acc:.3f}, P={ensemble_precision:.3f}, R={ensemble_recall:.3f}, F1={ensemble_f1:.3f}")

        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y_test, ensemble_pred)
        print(f"\n  í˜¼ë™ í–‰ë ¬:")
        print(f"           ì˜ˆì¸¡DOWN  ì˜ˆì¸¡UP")
        print(f"  ì‹¤ì œDOWN    {cm[0,0]:4d}    {cm[0,1]:4d}")
        if len(cm) > 1:
            print(f"  ì‹¤ì œUP      {cm[1,0]:4d}    {cm[1,1]:4d}")

        # ëª¨ë¸ ì €ì¥
        if ensemble_acc > 0.55:
            model_info = {
                'models': {name: r['model'] for name, r in results.items()},
                'scaler': scaler,
                'features': list(features.columns),
                'ensemble_accuracy': ensemble_acc,
                'ensemble_precision': ensemble_precision,
                'ensemble_recall': ensemble_recall,
                'ensemble_f1': ensemble_f1,
                'timeframe': timeframe,
                'data_size': len(X),
                'trained_at': datetime.now().isoformat()
            }

            filename = f"improved_{timeframe}_model.pkl"
            joblib.dump(model_info, f"models/{filename}")
            print(f"\n  âœ… ëª¨ë¸ ì €ì¥: models/{filename}")

            return model_info
        else:
            print(f"\n  âš ï¸ ì •í™•ë„ ë¶€ì¡± ({ensemble_acc:.1%})")
            return None

def main():
    trainer = ImprovedAccuracyTrainer()

    print("="*60)
    print("ğŸ”§ ê°œì„ ëœ ê³ ì •í™•ë„ ëª¨ë¸ í›ˆë ¨")
    print("ğŸ“Š 90ì¼ ë°ì´í„° ì‚¬ìš©")
    print("="*60)

    results = {}

    for timeframe in ['15m', '30m', '1h', '4h']:
        try:
            model_info = trainer.train_optimized_model(timeframe)
            if model_info:
                results[timeframe] = model_info
        except Exception as e:
            print(f"\nâŒ {timeframe} í›ˆë ¨ ì‹¤íŒ¨: {e}")

    # ê²°ê³¼ ìš”ì•½
    if results:
        print("\n" + "="*60)
        print("ğŸ“‹ í›ˆë ¨ ê²°ê³¼ ìš”ì•½")
        print("="*60)

        for tf, info in results.items():
            print(f"\n{tf}:")
            print(f"  ì•™ìƒë¸” ì •í™•ë„: {info['ensemble_accuracy']*100:.1f}%")
            print(f"  F1 ì ìˆ˜: {info['ensemble_f1']:.3f}")
            print(f"  ë°ì´í„° í¬ê¸°: {info['data_size']}ê°œ")

if __name__ == "__main__":
    main()